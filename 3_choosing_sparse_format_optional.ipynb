{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-poster')\n",
    "sns.set_palette('Set1', 10, desat=0.75)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from scipy import sparse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/'\n",
    "data = pd.read_csv(os.path.join(DATA_PATH, 'train_preprocessed.csv'), sep=';')\n",
    "data.fillna('xxx', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 µs, sys: 48 µs, total: 69 µs\n",
      "Wall time: 73 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgb\n",
    "\n",
    "def get_lgb_holdout_score(data_sparse, Y):\n",
    "    \"\"\"\n",
    "    input: csr sparse matrix and labels in a list/array/pd.Series format\n",
    "    output: float, log loss on holdout dataset\n",
    "    \n",
    "    Sparse matrix is splitted for train and holdout by 75 and 25% respectievly. Then light gbm model with\n",
    "    default parameters is trained on train with validation on holdout (which is strictly speaking is overfitting, but\n",
    "    it doesn't matter for now). Returned value is log loss perfomance of trained model on holdout set.\n",
    "    \"\"\"\n",
    "    train, valid, ytrain, yvalid = train_test_split(data_sparse, Y, train_size=0.75, random_state=0)\n",
    "    dtrain = lgb.Dataset(train, ytrain)\n",
    "    deval  = lgb.Dataset(valid, yvalid, reference=dtrain)\n",
    "    \n",
    "    params = {'task': 'train','boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'binary_logloss'}\n",
    "    \n",
    "    gbm = lgb.train(params,\n",
    "                dtrain,\n",
    "                num_boost_round=10000,\n",
    "                valid_sets=[dtrain, deval],\n",
    "                verbose_eval=False,\n",
    "                early_stopping_rounds=10)\n",
    "    lgb_pred = gbm.predict(valid, num_iteration=gbm.best_iteration)\n",
    "    return round(log_loss(yvalid, lgb_pred), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can think of three differnet sparse representations for this task:\n",
    "\n",
    "1) The most obvious way: horizontally concatenate two sparse matrices (TfidfVectorizer output), each representing one question. We'll have **doubled dimensionality**, and the decision tree will have to make **two splits** in order to figure out, that two questions have (or not have) particular word in common. That's likely will work... ok.\n",
    "\n",
    "2) We can also concatenate horizontally two sparse matrices (TfidfVectorizer output), one representing intersection quesitons, and the second - difference. **Dimensionality will be doubled**, just as in previous case, but it'll take just **one split** for decision tree to check, whether two questions have certain word in common or not. This will likely work better than the first option.\n",
    "\n",
    "3) Finally we can take two sparse matrices, each one representing one question (CountVectorizer), and take their sum. If we do this with binary CountVectorizer, we'll have matrix with **single dimensionality** (which is good, especially for decision-tree based algorithm). Each element in the matrix will one of three values:\n",
    "\n",
    "    a) 0 - column word is in neither question.\n",
    "    b) 1 - column word is in one question.\n",
    "    c) 2 - column word is in both questions.\n",
    "   \n",
    "This way our decision tree will be able to check any word in **one split**. Although the information about word count will be lost.\n",
    "\n",
    "4) The same as number 3, but before adding two sparse matrices, we'll multiply the second one by 2. This way each element in the matrix will one of four values:\n",
    "\n",
    "    a) 0 - column word is in neither question.\n",
    "    b) 1 - column word is in first question.\n",
    "    c) 2 - column word is in second questions.\n",
    "    d) 3 - column word in both questions.\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "Most likely, the best results will be yeilded by second or fourth option. Let's test it on stemmed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First option number of columns: 79676\n",
      "First option log loss: 0.3788 \n",
      "\n",
      "Second option number of columns: 79676\n",
      "Second option log loss: 0.3544 \n",
      "\n",
      "Third option number of columns: 39838\n",
      "Third option log loss: 0.3666 \n",
      "\n",
      "Fourth option number of columns: 39838\n",
      "Fourth option log loss: 0.3662 \n",
      "\n",
      "CPU times: user 2h 40min 59s, sys: 15min 39s, total: 2h 56min 38s\n",
      "Wall time: 37min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p_text = '_src'\n",
    "corpus = data['q1'+p_text].tolist() + data['q2'+p_text].tolist()\n",
    "\n",
    "# First option\n",
    "tfidf = TfidfVectorizer(max_df=0.8, min_df=3).fit(corpus) \n",
    "data_sparse = sparse.hstack([tfidf.transform(data['q1'+p_text])\n",
    "                            ,tfidf.transform(data['q2'+p_text])\n",
    "                            ], format = 'csr')   \n",
    "print ('First option number of columns:', data_sparse.shape[1])\n",
    "print ('First option log loss:', get_lgb_holdout_score(data_sparse, data.target), '\\n')\n",
    "\n",
    "# Second option\n",
    "tfidf = TfidfVectorizer(max_df=0.8, min_df=3).fit(corpus) \n",
    "data_sparse = sparse.hstack([tfidf.transform(data['inter'+p_text])\n",
    "                            ,tfidf.transform(data['extra'+p_text])\n",
    "                            ], format = 'csr')\n",
    "print ('Second option number of columns:', data_sparse.shape[1])\n",
    "print ('Second option log loss:', get_lgb_holdout_score(data_sparse, data.target), '\\n')\n",
    "\n",
    "# Third option\n",
    "CV = CountVectorizer(max_df=0.8, min_df=3, binary=True).fit(corpus) \n",
    "data_sparse = sparse.hstack([CV.transform(data['q1'+p_text]) \n",
    "                            +CV.transform(data['q2'+p_text])\n",
    "                            ], format = 'csr').astype(float)\n",
    "print ('Third option number of columns:', data_sparse.shape[1])\n",
    "print ('Third option log loss:', get_lgb_holdout_score(data_sparse, data.target), '\\n')\n",
    "\n",
    "# Fourth option\n",
    "CV = CountVectorizer(max_df=0.8, min_df=3, binary=True).fit(corpus) \n",
    "data_sparse = sparse.hstack([CV.transform(data['q1'+p_text]) \n",
    "                            +CV.transform(data['q2'+p_text])*2\n",
    "                            ], format = 'csr').astype(float)\n",
    "print ('Fourth option number of columns:', data_sparse.shape[1])\n",
    "print ('Fourth option log loss:', get_lgb_holdout_score(data_sparse, data.target), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first option is the worst, and the second option yeilds the best results. Let's try different preprocessings with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second option no stopwords log loss: 0.3775 \n",
      "\n",
      "Second option unprocessed log loss: 0.3544 \n",
      "\n",
      "Second option stemmed log loss: 0.3482 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Second option no stopwords\n",
    "p_text = '_nostops'\n",
    "corpus = data['q1'+p_text].tolist() + data['q2'+p_text].tolist()\n",
    "tfidf = TfidfVectorizer(max_df=0.8, min_df=3).fit(corpus) \n",
    "data_sparse = sparse.hstack([tfidf.transform(data['inter'+p_text])\n",
    "                            ,tfidf.transform(data['extra'+p_text])\n",
    "                            ], format = 'csr')\n",
    "print ('Second option no stopwords log loss:', get_lgb_holdout_score(data_sparse, data.target), '\\n')\n",
    "\n",
    "# Second option unprocessed\n",
    "p_text = '_src'\n",
    "corpus = data['q1'+p_text].tolist() + data['q2'+p_text].tolist()\n",
    "tfidf = TfidfVectorizer(max_df=0.8, min_df=3).fit(corpus) \n",
    "data_sparse = sparse.hstack([tfidf.transform(data['inter'+p_text])\n",
    "                            ,tfidf.transform(data['extra'+p_text])\n",
    "                            ], format = 'csr')\n",
    "print ('Second option unprocessed log loss:', get_lgb_holdout_score(data_sparse, data.target), '\\n')\n",
    "\n",
    "# Second option stemmed words\n",
    "p_text = '_stem'\n",
    "corpus = data['q1'+p_text].tolist() + data['q2'+p_text].tolist()\n",
    "tfidf = TfidfVectorizer(max_df=0.8, min_df=3).fit(corpus) \n",
    "data_sparse = sparse.hstack([tfidf.transform(data['inter'+p_text])\n",
    "                            ,tfidf.transform(data['extra'+p_text])\n",
    "                            ], format = 'csr')\n",
    "print ('Second option stemmed log loss:', get_lgb_holdout_score(data_sparse, data.target), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmed data shows the best results. Probabliy stopwords contain significant amount of information and raw questions on the contrary have too uch noise. Let's try to concatenate tfidf on part of speech tags and rerun the model. Since data_sparse is already built on stemmed data, we don't have to refit tfidf on stemmed data again. Let's just concatenate the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second option stemmed with tags log loss: 0.3404 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_text = '_tags'\n",
    "corpus = data['q1'+p_text].tolist() + data['q2'+p_text].tolist()\n",
    "tfidf = TfidfVectorizer(max_df=0.8, min_df=3).fit(corpus) \n",
    "data_sparse = sparse.hstack([data_sparse\n",
    "                            ,tfidf.transform(data['inter'+p_text])\n",
    "                            ,tfidf.transform(data['extra'+p_text])\n",
    "                            ], format='csr')\n",
    "print ('Second option stemmed with tags log loss:', get_lgb_holdout_score(data_sparse, data.target), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
