{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-poster')\n",
    "sns.set_palette('Set1', 10, desat=0.75)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "DATA_PATH = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 s, sys: 792 ms, total: 3.35 s\n",
      "Wall time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def load_sparse_csr(filename):\n",
    "    \"Loads scipy sparse matrix with csr format\"\n",
    "    loader = np.load(filename)\n",
    "    return sparse.csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])\n",
    "\n",
    "ytrain = pd.read_csv(os.path.join(DATA_PATH, 'target.csv')).target#[:10000]\n",
    "ids = pd.read_csv(os.path.join(DATA_PATH, 'train_ids.csv'), usecols=['graph_id'])#[:10000]\n",
    "\n",
    "data = load_sparse_csr(os.path.join(DATA_PATH, 'data_tfidf_stem_tags.npz'))#[:10000]\n",
    "kagg = load_sparse_csr(os.path.join(DATA_PATH, 'kagg_tfidf_stem_tags.npz'))#[:10000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "from fastFM.sgd import FMClassification\n",
    "\n",
    "def lgb_pred(params, train, ytrain, valid, yvalid, test, kagg):\n",
    "    train  = lgb.Dataset(train, ytrain)\n",
    "    dvalid = lgb.Dataset(valid, yvalid, reference=train)\n",
    "    \n",
    "    gbm = lgb.train(params,train,\n",
    "                    num_boost_round=100000,\n",
    "                    valid_sets=[train, dvalid],\n",
    "                    verbose_eval=False,\n",
    "                    early_stopping_rounds=20)\n",
    "    \n",
    "    fold_pred  = gbm.predict(test,  num_iteration=gbm.best_iteration)\n",
    "    kagg_pred  = gbm.predict(kagg,  num_iteration=gbm.best_iteration) \n",
    "    return fold_pred, kagg_pred\n",
    "\n",
    "def fastfm_pred(params, train, ytrain, test, kagg):\n",
    "    fmc = FMClassification(**params)\n",
    "    fmc.fit(train, ytrain)\n",
    "    fold_pred = fmc.predict_proba(test)\n",
    "    kagg_pred = fmc.predict_proba(kagg)\n",
    "    return fold_pred, kagg_pred\n",
    "        \n",
    "def get_oofs(model, data, kagg, y, ids, \n",
    "             n_splits=5, iters_total=3, params=None):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        model: string ('lgb'/'fastfm'/'xgb'/'nnet') or any sklearn model\n",
    "        data: train data in format of pandas DataFrame / numpay array / csr sparse matrix. All columns will be \n",
    "            used as features. Must not contain target or ids or non-numeric columns\n",
    "        kagg: test data in format of pandas DataFrame / numpay array / csr sparse matrix. All columns will be \n",
    "            used as features. Must not contain target or ids or non-numeric columns\n",
    "        y: target for train data in a form of pandas DataFrame / pandas Series / numpay array\n",
    "        ids: graph ids in a form of pandas Series. Graphs ids are used to split train data into separate graphs \n",
    "            to prevent overfitting. In other words, quesitons of the same graph will always be in one fold.\n",
    "        n_splits: number of splits for train data. Default value is 5. In order to get one OOF prediciton, \n",
    "            model must be fitted n_splits times\n",
    "        iters_total: number of total iterations. Default value is 3. OOFs then will be blended. \n",
    "            Total number of times a model will be fitted is n_splits*iters_total.\n",
    "        params: model parameters in a form of dictionary.\n",
    "    output: \n",
    "        data_oofs: numpy array of OOF predictions for train data. Result is blended iters_total times\n",
    "        kagg_oofs: numpy array of predictions for train data. Result is blended n_splits*iters_total times\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(data).__name__=='DataFrame':\n",
    "        data = data.values\n",
    "    if type(kagg).__name__=='DataFrame':\n",
    "        kagg = kagg.values\n",
    "    if model == 'fastfm':\n",
    "        y = y.replace(0, -1)\n",
    "        if type(data).__name__ != 'csr_matrix':\n",
    "            data = sparse.csr_matrix(data)\n",
    "            kagg = sparse.csr_matrix(kagg)\n",
    "    if type(y).__name__=='Series' or type(y).__name__=='DataFrame':\n",
    "        y = y.values\n",
    "    \n",
    "    # matrices to store preditions\n",
    "    data_oofs = np.zeros((data.shape[0]))\n",
    "    kagg_oofs = np.zeros((kagg.shape[0]))\n",
    "    \n",
    "    graph_ids_unique = ids.graph_id.unique()\n",
    "    \n",
    "    for iter_num in range(iters_total):\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "        for train_graphs, test_graphs  in kf.split(graph_ids_unique):\n",
    "            train_ind = ids[ids.graph_id.isin(graph_ids_unique[train_graphs])].index.values\n",
    "            test_ind  = ids[ids.graph_id.isin(graph_ids_unique[test_graphs ])].index.values\n",
    "            \n",
    "            # Adding validation sets (from train set) for models, that require it. \n",
    "            # Validation size is 12.5% of train fold.\n",
    "            if model=='xgb' or model=='lgb' or model=='nnet':\n",
    "                kf_valid = KFold(n_splits=8, shuffle=True)\n",
    "                graph_ids_train = graph_ids_unique[train_graphs]\n",
    "                train_graphs, valid_graphs = list(kf_valid.split(graph_ids_train))[0]\n",
    "                train_ind = ids[ids.graph_id.isin(graph_ids_train[train_graphs])].index.values\n",
    "                valid_ind = ids[ids.graph_id.isin(graph_ids_train[valid_graphs])].index.values\n",
    "                \n",
    "            if model=='lgb':\n",
    "                fold_pred, kagg_pred = lgb_pred (params, data[train_ind], y[train_ind], \n",
    "                                                         data[valid_ind], y[valid_ind],\n",
    "                                                         data[test_ind], kagg)\n",
    "            elif model=='xgb':\n",
    "                fold_pred, kagg_pred = xgb_pred (params, data[train_ind], y[train_ind], \n",
    "                                                         data[valid_ind], y[valid_ind],\n",
    "                                                         data[test_ind], kagg)\n",
    "            elif model=='nnet':\n",
    "                fold_pred, kagg_pred = nnet_pred(params, data[train_ind], y[train_ind], \n",
    "                                                         data[valid_ind], y[valid_ind],\n",
    "                                                         data[test_ind], kagg)\n",
    "            elif model=='fastfm':\n",
    "                fold_pred, kagg_pred = fastfm_pred(params, data[train_ind], y[train_ind], data[test_ind], kagg)\n",
    "               \n",
    "            # Block for working with sklearn models\n",
    "            else:\n",
    "                model.fit(data[train_ind], y[train_ind])\n",
    "                try:\n",
    "                    fold_pred = model.predict_proba(data[test_ind])[:,1]\n",
    "                    kagg_pred = model.predict_proba(kagg)[:,1]\n",
    "                except:\n",
    "                    try:\n",
    "                        fold_pred = model.predict_proba(data[test_ind])\n",
    "                        kagg_pred = model.predict_proba(kagg)\n",
    "                    except:\n",
    "                        fold_pred = model.predict(data[test_ind])\n",
    "                        kagg_pred = model.predict(kagg)\n",
    "            \n",
    "            \n",
    "            data_oofs[test_ind] += fold_pred\n",
    "            kagg_oofs += kagg_pred\n",
    "            print ('fold loss: ', log_loss(y[test_ind], fold_pred))            \n",
    "        \n",
    "        print ('iteration OOF score:', log_loss(y, data_oofs/(iter_num+1)))\n",
    "    data_oofs /= iters_total\n",
    "    kagg_oofs /= (iters_total*n_splits)\n",
    "    return data_oofs, kagg_oofs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOF predicitons with LightGBM on TF-IDF transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold loss:  0.39505452836\n",
      "fold loss:  0.391707470137\n",
      "fold loss:  0.403296928625\n",
      "fold loss:  0.394772659807\n",
      "fold loss:  0.402759217594\n",
      "iteration OOF score: 0.39760496311\n",
      "fold loss:  0.39553976321\n",
      "fold loss:  0.396333165703\n",
      "fold loss:  0.408735197222\n",
      "fold loss:  0.396774328361\n",
      "fold loss:  0.394911306052\n",
      "iteration OOF score: 0.391597208616\n",
      "fold loss:  0.391230999644\n",
      "fold loss:  0.398633593536\n",
      "fold loss:  0.39557736232\n",
      "fold loss:  0.398128801855\n",
      "fold loss:  0.400803811999\n",
      "iteration OOF score: 0.389326754038\n",
      "lgb_tfidf_oof is done in 116.8 minutes \n",
      "\n",
      "fold loss:  0.560250481386\n",
      "fold loss:  0.561786047197\n",
      "fold loss:  0.555596095719\n",
      "fold loss:  0.56354098972\n",
      "fold loss:  0.575602435166\n",
      "iteration OOF score: 0.563433610872\n",
      "fold loss:  0.564527125784\n",
      "fold loss:  0.565327783431\n",
      "fold loss:  0.55716693628\n",
      "fold loss:  0.558937258263\n",
      "fold loss:  0.568529505479\n",
      "iteration OOF score: 0.559943220869\n",
      "fold loss:  0.558031928643\n",
      "fold loss:  0.561895321244\n",
      "fold loss:  0.559661966874\n",
      "fold loss:  0.569648454457\n",
      "fold loss:  0.559720892525\n",
      "iteration OOF score: 0.558320344518\n",
      "lgb_tfidfpca_oof is done in 9.3 minutes \n",
      "\n",
      "CPU times: user 11h 22min 52s, sys: 34min 13s, total: 11h 57min 5s\n",
      "Wall time: 2h 6min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# PCA transofmed matrices\n",
    "pca = TruncatedSVD(n_components=20)\n",
    "pca.fit(data)\n",
    "data_pca = pca.transform(data)\n",
    "kagg_pca = pca.transform(kagg)\n",
    "\n",
    "# Empty dfs for oof preditions\n",
    "data_preds = pd.DataFrame()\n",
    "kagg_preds = pd.DataFrame()\n",
    "\n",
    "# LightGBM hyperparameters\n",
    "params = {\n",
    "        'task': 'train','boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'binary_logloss',\n",
    "        'feature_fraction': 0.95,\n",
    "        'min_data_in_leaf': 10, \n",
    "        'bagging_freq': 3, \n",
    "        'min_gain_to_split': 0, \n",
    "        'lambda_l2': 1, \n",
    "        'learning_rate': 0.075, \n",
    "        'num_leaves': 128, \n",
    "        'bagging_fraction': 0.85}\n",
    "\n",
    "t_start = time.time()\n",
    "data_preds['lgb_tfidf_oof'], kagg_preds['lgb_tfidf_oof'] = get_oofs('lgb'\n",
    "                                                                    ,data\n",
    "                                                                    ,kagg\n",
    "                                                                    ,ytrain\n",
    "                                                                    ,ids \n",
    "                                                                    ,params=params)\n",
    "print ('lgb_tfidf_oof is done in {} minutes \\n'.format(round((time.time()-t_start)/60,1) ))\n",
    "\n",
    "t_start = time.time()\n",
    "data_preds['lgb_tfidfpca_oof'], kagg_preds['lgb_tfidfpca_oof'] = get_oofs('lgb'\n",
    "                                                                            ,data_pca\n",
    "                                                                            ,kagg_pca\n",
    "                                                                            ,ytrain\n",
    "                                                                            ,ids\n",
    "                                                                            ,params=params)\n",
    "print ('lgb_tfidfpca_oof is done in {} minutes \\n'.format(round((time.time()-t_start)/60,1) ))\n",
    "\n",
    "\n",
    "data_preds.to_csv(os.path.join(DATA_PATH, 'train_lgb_tfidf_oof.csv'), index=False)\n",
    "kagg_preds.to_csv(os.path.join(DATA_PATH, 'test_lgb_tfidf_oof.csv' ), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Convolution1D, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam, SGD, Adadelta\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "def batch_generator(X, y, BATCH_SIZE, EPOCH_PARTION):\n",
    "    \"\"\"\n",
    "    Batch generator for nnet training\n",
    "    input:\n",
    "        X - train dataset,  numpy array or csr matrix\n",
    "        y - target, numpy array\n",
    "        BATCH_SIZE - int, number of objects in batch. If X is csr matrix, it will be transformed \n",
    "        to dense array so batch size must be small enough for this array to fit in memory\n",
    "        EPOCH_PARTION - float, share of objects that will be used for training in epoch\n",
    "        \n",
    "    Important: in this implementation each batch is constructed from random objects from train.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_number = 0\n",
    "    batches_per_epoch = int(X.shape[0]/BATCH_SIZE*EPOCH_PARTION)\n",
    "    while True:\n",
    "        batch_indexes = np.random.choice(X.shape[0], BATCH_SIZE)\n",
    "        if type(X).__name__ == 'csr_matrix':\n",
    "            X_batch = X[batch_indexes].toarray()\n",
    "        else:\n",
    "            X_batch = X[batch_indexes]\n",
    "        y_batch = to_categorical(y, num_classes=2)[batch_indexes]\n",
    "        batch_number += 1\n",
    "        yield X_batch, y_batch\n",
    "        if batch_number == batches_per_epoch-1:\n",
    "            batch_number = 0\n",
    "            \n",
    "def batch_generator_p(X, BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Batch generator for nnet predicitons\n",
    "    input:\n",
    "        X - train dataset,  numpy array or csr matrix\n",
    "        BATCH_SIZE - number of objects in batch. If X is csr matrix, it will be transformed \n",
    "        to dense array so batch size must be small enough for this array to fit in memory        \n",
    "    \"\"\"\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/BATCH_SIZE)\n",
    "    batch_number = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_indexes = sample_index[BATCH_SIZE*batch_number : BATCH_SIZE*(batch_number+1)]\n",
    "        if type(X).__name__ == 'csr_matrix':\n",
    "            X_batch = X[batch_indexes].toarray()\n",
    "        else:\n",
    "            X_batch = X[batch_indexes]\n",
    "        batch_number += 1\n",
    "        yield (X_batch)\n",
    "        if batch_number == number_of_batches:\n",
    "            batch_number = 0\n",
    "            \n",
    "def nnet(data, n1, n2, d1, d2, regul, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to compile simple nnet. Architecture is self-explanatory with code\n",
    "    input:\n",
    "        data - numpy arary or csr matrix for training\n",
    "        n1, n2 - ints, number of neurons in first and second layers\n",
    "        d1, d2 - float, dropouts in first and second layers\n",
    "        regul - float, regularization paramter, the same for both layers\n",
    "    output:\n",
    "        nnet model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    if regul>0:\n",
    "        model.add(Dense(n1, input_dim=data.shape[1], \n",
    "                        kernel_regularizer=l2(regul), \n",
    "                        activity_regularizer=l1(regul)))\n",
    "    else:    \n",
    "        model.add(Dense(n1, input_dim=data.shape[1]))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(d1))\n",
    "    \n",
    "    if regul>0:\n",
    "        model.add(Dense(n2, \n",
    "                        kernel_regularizer=l2(regul), \n",
    "                        activity_regularizer=l1(regul)))\n",
    "    else:    \n",
    "        model.add(Dense(n2))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(d2))\n",
    "\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = Adam()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['binary_crossentropy'])\n",
    "    return model\n",
    "\n",
    "def nnet_pred(params, train, ytrain, valid, yvalid, test, kagg):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if type(train).__name__ == 'csr_matrix':\n",
    "        scaler = MaxAbsScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "    \n",
    "    train = scaler.fit_transform(train)\n",
    "    valid = scaler.transform(valid)\n",
    "    test  = scaler.transform(test)\n",
    "    kagg  = scaler.transform(kagg)\n",
    "    \n",
    "    model = nnet(train, **params)\n",
    "    early_stopper = EarlyStopping(monitor='val_binary_crossentropy', patience=10, verbose=0, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(filepath='nnet_test.hdf5', monitor='val_binary_crossentropy', save_best_only=True)\n",
    "\n",
    "    BATCH_SIZE = params.get('BATCH_SIZE', 256)\n",
    "    EPOCH_PARTION = params.get('EPOCH_PARTION', 0.75)\n",
    "    \n",
    "    model.fit_generator(generator=batch_generator(train, ytrain, BATCH_SIZE, EPOCH_PARTION),\n",
    "                        samples_per_epoch=int(train.shape[0]/BATCH_SIZE*EPOCH_PARTION),\n",
    "                        verbose=0, nb_epoch=1000,\n",
    "\n",
    "                        validation_data=batch_generator(valid, yvalid, BATCH_SIZE, EPOCH_PARTION), \n",
    "                        #nb_val_samples=int(valid.shape[0]/BATCH_SIZE*EPOCH_PARTION)*2, \n",
    "                        validation_steps = int(valid.shape[0]/BATCH_SIZE),\n",
    "\n",
    "                        callbacks=[early_stopper, checkpoint])\n",
    "    \n",
    "    model.load_weights('nnet_test.hdf5') \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['binary_crossentropy'])\n",
    "    \n",
    "    fold_pred = model.predict_generator(generator=batch_generator_p(test, BATCH_SIZE), \n",
    "                                val_samples=test.shape[0]/BATCH_SIZE)\n",
    "    kagg_pred = model.predict_generator(generator=batch_generator_p(kagg, BATCH_SIZE), \n",
    "                                val_samples=kagg.shape[0]/BATCH_SIZE)\n",
    "    return fold_pred[:,1], kagg_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold loss:  0.422063976781\n",
      "fold loss:  0.446170907029\n",
      "fold loss:  0.427456399184\n",
      "fold loss:  0.429504116896\n",
      "fold loss:  0.446972422033\n",
      "iteration OOF score: 0.434428633895\n",
      "fold loss:  0.423245459602\n",
      "fold loss:  0.454989037373\n",
      "fold loss:  0.436115124954\n",
      "fold loss:  0.441851586837\n",
      "fold loss:  0.430626961851\n",
      "iteration OOF score: 0.41762264606\n",
      "fold loss:  0.426923811564\n",
      "fold loss:  0.458325888393\n",
      "fold loss:  0.422328773042\n",
      "fold loss:  0.432564368697\n",
      "fold loss:  0.432415393343\n",
      "iteration OOF score: 0.412751694357\n",
      "nnet_tfidf_oof is done in 2336.2 minutes \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_preds = pd.DataFrame()\n",
    "kagg_preds = pd.DataFrame()\n",
    "params = {'n1':512, 'n2':256, 'd1':0.5, 'd2':0.25, 'regul':1e-9, 'BATCH_SIZE':512, 'EPOCH_PARTION':0.75}\n",
    "\n",
    "\n",
    "t_start = time.time()\n",
    "data_preds['nnet_tfidf_oof'], kagg_preds['nnet_tfidf_oof'] = get_oofs('nnet'\n",
    "                                                            ,data\n",
    "                                                            ,kagg\n",
    "                                                            ,ytrain\n",
    "                                                            ,ids \n",
    "                                                            ,params=params)\n",
    "print ('nnet_tfidf_oof is done in {} minutes \\n'.format(round((time.time()-t_start)/60,1) ))\n",
    "\n",
    "\n",
    "data_preds.to_csv(os.path.join(DATA_PATH, 'train_nnet_tfidf_oof.csv'), index=False)\n",
    "kagg_preds.to_csv(os.path.join(DATA_PATH, 'test_nnet_tfidf_oof.csv' ), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOF predicitons with factorization machines on TF-IDF transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold loss:  0.65599898572\n",
      "fold loss:  0.647734455597\n",
      "fold loss:  0.650891742555\n",
      "fold loss:  0.698675414611\n",
      "fold loss:  0.664693375386\n",
      "iteration OOF score: 0.66414823069\n",
      "fold loss:  0.674839754699\n",
      "fold loss:  0.655822259507\n",
      "fold loss:  0.666759726946\n",
      "fold loss:  0.66029638867\n",
      "fold loss:  0.651864755985\n",
      "iteration OOF score: 0.662976429274\n",
      "fold loss:  0.654551261602\n",
      "fold loss:  0.671557997558\n",
      "fold loss:  0.665547201307\n",
      "fold loss:  0.666611129147\n",
      "fold loss:  0.65511445235\n",
      "iteration OOF score: 0.662868041762\n",
      "CPU times: user 2min 41s, sys: 23.2 s, total: 3min 4s\n",
      "Wall time: 3min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_preds = pd.DataFrame()\n",
    "kagg_preds = pd.DataFrame()\n",
    "\n",
    "params = {'n_iter':5000, 'init_stdev':5e-3, 'step_size':5e-3, 'l2_reg_w':10, 'l2_reg':10}\n",
    "data_preds['FM_oof'], kagg_preds['FM_oof'] = get_oofs('fastfm', data, kagg, ytrain, ids, params=params)\n",
    "\n",
    "data_preds.to_csv(os.path.join(DATA_PATH, 'train_fm_oof.csv'), index=False)\n",
    "kagg_preds.to_csv(os.path.join(DATA_PATH, 'test_fm_oof.csv' ), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from keras.engine import Model\n",
    "from keras.engine import Input\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from keras.layers import LSTM, concatenate\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.layers import Embedding\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_FOLDER = '../../data'\n",
    "TRAIN_MODE = True\n",
    "NUM_EPOCHS = 10\n",
    "NUM_FILTERS = 100\n",
    "KERNEL_SIZE = 3\n",
    "OUTPUT_SIZE = 128\n",
    "MAX_FEATURES = 200000\n",
    "EMBEDDING_SIZE = 300\n",
    "DROPOUT_PROB = 0.5\n",
    "MAX_LEN = 20\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE = 3\n",
    "WEIGHTS_NAME = 'weights_convnet.h5'\n",
    "COMPILED_MODEL_NAME = 'compiled_convnet_model.h5'\n",
    "TRAIN_OUTPUT_PROB_NAME = '../../data/train_convnet_proba.pkl'\n",
    "GLOVE_VECTORDS_PATH = '../../data/glove.840B.300d.txt'\n",
    "INPUT_TRAIN_FILE = '../../data/train.csv'\n",
    "INPUT_TEST_FILE = '../../data/test.csv'\n",
    "TEST_OUTPUT_PROB_NAME = '../../data/test_convnet_proba.pkl'\n",
    "BASE_DIR = '../../data/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin.gz'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'train.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'test.csv'\n",
    "\n",
    "# print('Processing text dataset')\n",
    "#\n",
    "#\n",
    "# # The function \"text_to_wordlist\" is from\n",
    "# # https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "# def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "#     # Clean the text, with the option to remove stopwords and to stem words.\n",
    "#\n",
    "#     # Convert words to lower case and split them\n",
    "#     text = text.lower().split()\n",
    "#\n",
    "#     # Optionally, remove stop words\n",
    "#     if remove_stopwords:\n",
    "#         stops = set(stopwords.words(\"english\"))\n",
    "#         text = [w for w in text if not w in stops]\n",
    "#\n",
    "#     text = \" \".join(text)\n",
    "#\n",
    "#     # Clean the text\n",
    "#     text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "#     text = re.sub(r\"what's\", \"what is \", text)\n",
    "#     text = re.sub(r\"\\'s\", \" \", text)\n",
    "#     text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "#     text = re.sub(r\"can't\", \"cannot \", text)\n",
    "#     text = re.sub(r\"n't\", \" not \", text)\n",
    "#     text = re.sub(r\"i'm\", \"i am \", text)\n",
    "#     text = re.sub(r\"\\'re\", \" are \", text)\n",
    "#     text = re.sub(r\"\\'d\", \" would \", text)\n",
    "#     text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "#     text = re.sub(r\",\", \" \", text)\n",
    "#     text = re.sub(r\"\\.\", \" \", text)\n",
    "#     text = re.sub(r\"!\", \" ! \", text)\n",
    "#     text = re.sub(r\"\\/\", \" \", text)\n",
    "#     text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "#     text = re.sub(r\"\\+\", \" + \", text)\n",
    "#     text = re.sub(r\"\\-\", \" - \", text)\n",
    "#     text = re.sub(r\"\\=\", \" = \", text)\n",
    "#     text = re.sub(r\"'\", \" \", text)\n",
    "#     text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "#     text = re.sub(r\":\", \" : \", text)\n",
    "#     text = re.sub(r\" e g \", \" eg \", text)\n",
    "#     text = re.sub(r\" b g \", \" bg \", text)\n",
    "#     text = re.sub(r\" u s \", \" american \", text)\n",
    "#     text = re.sub(r\"\\0s\", \"0\", text)\n",
    "#     text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "#     text = re.sub(r\"e - mail\", \"email\", text)\n",
    "#     text = re.sub(r\"j k\", \"jk\", text)\n",
    "#     text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "#\n",
    "#     # Optionally, shorten words to their stems\n",
    "#     if stem_words:\n",
    "#         text = text.split()\n",
    "#         stemmer = SnowballStemmer('english')\n",
    "#         stemmed_words = [stemmer.stem(word) for word in text]\n",
    "#         text = \" \".join(stemmed_words)\n",
    "#\n",
    "#     # Return a list of words\n",
    "#     return (text)\n",
    "#\n",
    "#\n",
    "# texts_1 = []\n",
    "# texts_2 = []\n",
    "# YTrain = []\n",
    "# with codecs.open(TRAIN_DATA_FILE) as f:\n",
    "#     reader = csv.reader(f, delimiter=',')\n",
    "#     header = next(reader)\n",
    "#     for values in reader:\n",
    "#         texts_1.append(text_to_wordlist(values[3]))\n",
    "#         texts_2.append(text_to_wordlist(values[4]))\n",
    "#         YTrain.append(int(values[5]))\n",
    "# print('Found %s texts in train.csv' % len(texts_1))\n",
    "#\n",
    "# test_texts_1 = []\n",
    "# test_texts_2 = []\n",
    "# test_ids = []\n",
    "# with codecs.open(TEST_DATA_FILE) as f:\n",
    "#     reader = csv.reader(f, delimiter=',')\n",
    "#     header = next(reader)\n",
    "#     for values in reader:\n",
    "#         test_texts_1.append(text_to_wordlist(values[1]))\n",
    "#         test_texts_2.append(text_to_wordlist(values[2]))\n",
    "#         test_ids.append(values[0])\n",
    "# print('Found %s texts in test.csv' % len(test_texts_1))\n",
    "#\n",
    "# tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "# tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "#\n",
    "# sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "# sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "# test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "# test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "#\n",
    "# word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens' % len(word_index))\n",
    "#\n",
    "# x1_train = pad_sequences(sequences_1, maxlen=MAX_LEN)\n",
    "# x2_train = pad_sequences(sequences_2, maxlen=MAX_LEN)\n",
    "# YTrain = np.array(YTrain)\n",
    "# print('Shape of data tensor:', x1_train.shape)\n",
    "# print('Shape of label tensor:', YTrain.shape)\n",
    "#\n",
    "# x1_test = pad_sequences(test_sequences_1, maxlen=MAX_LEN)\n",
    "# x2_test = pad_sequences(test_sequences_2, maxlen=MAX_LEN)\n",
    "# test_ids = np.array(test_ids)\n",
    "#\n",
    "# nb_words = min(MAX_FEATURES, len(word_index)) + 1\n",
    "#\n",
    "# with open('dump_sequences.pkl', 'wb') as f:\n",
    "#     pickle.dump([x1_train, x2_train, x1_test, x2_test, nb_words], f)\n",
    "#\n",
    "# print('Preparing embedding matrix')\n",
    "# word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "# print('Found %s word vectors of word2vec' % len(word2vec.vocab))\n",
    "# embedding_matrix = np.zeros((nb_words, EMBEDDING_SIZE))\n",
    "# for word, i in word_index.items():\n",
    "#     if word in word2vec.vocab:\n",
    "#         embedding_matrix[i] = word2vec.word_vec(word)\n",
    "# print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "# np.save('embedding_matrix', embedding_matrix)\n",
    "\n",
    "import cPickle as pickle\n",
    "with open('dump_sequences.pkl', 'rb') as f:\n",
    "    x1_train, x2_train, x1_test, x2_test, nb_words = pickle.load(f)\n",
    "embedding_matrix = np.load('embedding_matrix.npy')\n",
    "YTrain = pd.read_csv(INPUT_FOLDER + '/train.csv', usecols=['is_duplicate']).values.ravel()\n",
    "\n",
    "class KerasConvnet:\n",
    "    def __init__(self, use_weights=None):\n",
    "\n",
    "        num_lstm = 267\n",
    "        num_dense = 117\n",
    "        rate_drop_lstm = 0.25\n",
    "        rate_drop_dense = 0.15\n",
    "\n",
    "        act = 'relu'\n",
    "\n",
    "        embedding_layer = Embedding(nb_words,\n",
    "                                    EMBEDDING_SIZE,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_LEN,\n",
    "                                    trainable=False)\n",
    "        lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "        sequence_1_input = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "        embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "        x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "        sequence_2_input = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "        embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "        y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "        merged = concatenate([x1, y1])\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        merged = Dense(num_dense, activation=act)(merged)\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "        preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "        model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n",
    "\n",
    "        if use_weights is not None:\n",
    "            model.load_weights(use_weights)\n",
    "            \n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='nadam',\n",
    "                      metrics=['acc'])\n",
    "        model.summary()\n",
    "\n",
    "        self.model = model\n",
    "        self.earlystop = EarlyStopping(monitor='val_loss', patience=3, verbose=0)\n",
    "        self.checkpoint = ModelCheckpoint(WEIGHTS_NAME, monitor='val_loss', save_best_only=True, verbose=2)\n",
    "        self.class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "\n",
    "    def fit(self, train1, train2, y_train):\n",
    "        print 'Fitting ... '\n",
    "        input_arrays = [train1, train2]\n",
    "        self.model.fit(input_arrays, y_train, batch_size=BATCH_SIZE,\n",
    "                       epochs=NUM_EPOCHS, validation_split=0.1, shuffle=True,\n",
    "                       callbacks=[self.earlystop, self.checkpoint])\n",
    "\n",
    "    def predict(self, test1, test2):\n",
    "        input_arrays = [test1, test2]\n",
    "        preds = self.model.predict(input_arrays, batch_size=BATCH_SIZE, verbose=1)\n",
    "        for _ in range(4):\n",
    "            preds += self.model.predict(input_arrays, batch_size=BATCH_SIZE, verbose=1)\n",
    "        preds /= 5.\n",
    "\n",
    "        return preds\n",
    "\n",
    "print x1_train.shape, x2_train.shape\n",
    "print x1_test.shape, x2_test.shape\n",
    "print nb_words\n",
    "\n",
    "N_FOLDS = 3\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS).split(np.zeros((x1_train.shape[0], 1)), YTrain)\n",
    "splits = list(skf)\n",
    "\n",
    "print \"Creating train and test sets for blending.\"\n",
    "\n",
    "preds_train = np.zeros((x1_train.shape[0],))\n",
    "loglosses = []\n",
    "\n",
    "preds_test_j = np.zeros((x1_test.shape[0], N_FOLDS))\n",
    "for fold_id, (train_indexes, predict_indexes) in enumerate(splits):\n",
    "    print \"Fold\", fold_id\n",
    "\n",
    "    clf = KerasConvnet()\n",
    "    clf.fit(x1_train[train_indexes], x2_train[train_indexes], YTrain[train_indexes])\n",
    "\n",
    "    y_pred = clf.predict(x1_train[predict_indexes], x2_train[predict_indexes])\n",
    "    preds_train[predict_indexes] = y_pred\n",
    "\n",
    "    lloss = log_loss(YTrain[predict_indexes], y_pred)\n",
    "    loglosses.append(lloss)\n",
    "    print 'LogLoss: ', lloss\n",
    "\n",
    "    # Predict on entire test set\n",
    "    preds_test_j[:, fold_id] = clf.predict(x1_test, x2_test).ravel()\n",
    "\n",
    "print \"Out of fold logloss-es:\\n\", loglosses\n",
    "\n",
    "# Save OOF predictions as features\n",
    "pd.DataFrame(preds_train, columns=['siamese_oof']).to_csv(INPUT_FOLDER + '/features/siamese_oof_train.csv')\n",
    "\n",
    "# Save mean of predictions for test\n",
    "preds_test = preds_test_j.mean(1)\n",
    "pd.DataFrame(preds_test, columns=['siamese_oof']).to_csv(INPUT_FOLDER + '/features/siamese_oof_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
