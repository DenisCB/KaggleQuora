{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-poster')\n",
    "sns.set_palette('Set1', 10, desat=0.75)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "DATA_PATH = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.66 s, sys: 992 ms, total: 3.66 s\n",
      "Wall time: 3.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def load_sparse_csr(filename):\n",
    "    \"Loads scipy sparse matrix with csr format\"\n",
    "    loader = np.load(filename)\n",
    "    return csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])\n",
    "\n",
    "ytrain = pd.read_csv(os.path.join(DATA_PATH, 'final/target.csv')).target[:100000]\n",
    "ids = pd.read_csv(os.path.join(DATA_PATH, 'final/data_ids.csv'), usecols=['graph_id'])[:100000]\n",
    "\n",
    "data = load_sparse_csr(os.path.join(DATA_PATH, 'data_tfidf_stem_tags.npz'))[:100000]\n",
    "kagg = load_sparse_csr(os.path.join(DATA_PATH, 'kagg_tfidf_stem_tags.npz'))[:100000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "from fastFM.sgd import FMClassification\n",
    "\n",
    "def lgb_pred(params, train, ytrain, valid, yvalid, test, kagg):\n",
    "    train  = lgb.Dataset(train, ytrain)\n",
    "    dvalid = lgb.Dataset(valid, yvalid, reference=train)\n",
    "    \n",
    "    gbm = lgb.train(params,train,\n",
    "                    num_boost_round=100000,\n",
    "                    valid_sets=[train, dvalid],\n",
    "                    verbose_eval=False,\n",
    "                    early_stopping_rounds=20)\n",
    "    \n",
    "    fold_pred  = gbm.predict(test,  num_iteration=gbm.best_iteration)\n",
    "    kagg_pred  = gbm.predict(kagg,  num_iteration=gbm.best_iteration) \n",
    "    return fold_pred, kagg_pred\n",
    "\n",
    "def fastfm_pred(params, train, ytrain, test, kagg):\n",
    "    fmc = FMClassification(**params)\n",
    "    fmc.fit(train, ytrain)\n",
    "    fold_pred = fmc.predict_proba(test)\n",
    "    kagg_pred = fmc.predict_proba(kagg)\n",
    "    return fold_pred, kagg_pred\n",
    "        \n",
    "def get_oofs(model, data, kagg, y, ids, n_splits=5, iters_total=3, params=None):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        model: string ('lgb'/'fastfm'/'xgb'/'nnet') or any sklearn model\n",
    "        data: train data in format of pandas DataFrame / numpay array / csr sparse matrix. All columns will be \n",
    "            used as features. Must not contain target or ids or non-numeric columns\n",
    "        kagg: test data in format of pandas DataFrame / numpay array / csr sparse matrix. All columns will be \n",
    "            used as features. Must not contain target or ids or non-numeric columns\n",
    "        y: target for train data in a form of pandas DataFrame / pandas Series / numpay array\n",
    "        ids: graph ids in a form of pandas Series. Graphs ids are used to split train data into separate graphs \n",
    "            to prevent overfitting. In other words, quesitons of the same graph will always be in one fold.\n",
    "        n_splits: number of splits for train data. Default value is 5. In order to get one OOF prediciton, \n",
    "            model must be fitted n_splits times\n",
    "        iters_total: number of total iterations. Default value is 3. OOFs then will be blended. \n",
    "            Total number of times a model will be fitted is n_splits*iters_total.\n",
    "        params: model parameters in a form of dictionary.\n",
    "    output: \n",
    "        data_oofs: numpy array of OOF predictions for train data. Result is blended iters_total times\n",
    "        kagg_oofs: numpy array of predictions for train data. Result is blended n_splits*iters_total times\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(data).__name__=='DataFrame':\n",
    "        data = data.values\n",
    "    if type(kagg).__name__=='DataFrame':\n",
    "        kagg = kagg.values\n",
    "    if model == 'fastfm':\n",
    "        y = y.replace(0, -1)\n",
    "        if type(data).__name__ != 'csr_matrix':\n",
    "            data = sparse.csr_matrix(data)\n",
    "            kagg = sparse.csr_matrix(kagg)\n",
    "    if type(y).__name__=='Series' or type(y).__name__=='DataFrame':\n",
    "        y = y.values\n",
    "    \n",
    "    # matrices to store preditions\n",
    "    data_oofs = np.zeros((data.shape[0]))\n",
    "    kagg_oofs = np.zeros((kagg.shape[0]))\n",
    "    \n",
    "    graph_ids_unique = ids.graph_id.unique()\n",
    "    \n",
    "    for iter_num in range(iters_total):\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "        for train_graphs, test_graphs  in kf.split(graph_ids_unique):\n",
    "            train_ind = ids[ids.graph_id.isin(graph_ids_unique[train_graphs])].index.values\n",
    "            test_ind  = ids[ids.graph_id.isin(graph_ids_unique[test_graphs ])].index.values\n",
    "            \n",
    "            # Adding validation sets (from train set) for models, that require it. \n",
    "            # Validation size is 12.5% of train fold.\n",
    "            if model=='xgb' or model=='lgb' or model=='nnet':\n",
    "                kf_valid = KFold(n_splits=8, shuffle=True)\n",
    "                graph_ids_train = graph_ids_unique[train_graphs]\n",
    "                train_graphs, valid_graphs = list(kf_valid.split(graph_ids_train))[0]\n",
    "                train_ind = ids[ids.graph_id.isin(graph_ids_train[train_graphs])].index.values\n",
    "                valid_ind = ids[ids.graph_id.isin(graph_ids_train[valid_graphs])].index.values\n",
    "                \n",
    "            if model=='lgb':\n",
    "                fold_pred, kagg_pred = lgb_pred (params, data[train_ind], y[train_ind], \n",
    "                                                         data[valid_ind], y[valid_ind],\n",
    "                                                         data[test_ind], kagg)\n",
    "            elif model=='xgb':\n",
    "                fold_pred, kagg_pred = xgb_pred (params, data[train_ind], y[train_ind], \n",
    "                                                         data[valid_ind], y[valid_ind],\n",
    "                                                         data[test_ind], kagg)\n",
    "            elif model=='nnet':\n",
    "                fold_pred, kagg_pred = nnet_pred(params, data[train_ind], y[train_ind], \n",
    "                                                         data[valid_ind], y[valid_ind],\n",
    "                                                         data[test_ind], kagg)\n",
    "            elif model=='fastfm':\n",
    "                fold_pred, kagg_pred = fastfm_pred(params, data[train_ind], y[train_ind], data[test_ind], kagg)\n",
    "               \n",
    "            # Block for working with sklearn models\n",
    "            else:\n",
    "                model.fit(data[train_ind], y[train_ind])\n",
    "                try:\n",
    "                    fold_pred = model.predict_proba(data[test_ind])[:,1]\n",
    "                    kagg_pred = model.predict_proba(kagg)[:,1]\n",
    "                except:\n",
    "                    try:\n",
    "                        fold_pred = model.predict_proba(data[test_ind])\n",
    "                        kagg_pred = model.predict_proba(kagg)\n",
    "                    except:\n",
    "                        fold_pred = model.predict(data[test_ind])\n",
    "                        kagg_pred = model.predict(kagg)\n",
    "            \n",
    "            \n",
    "            data_oofs[test_ind] += fold_pred\n",
    "            kagg_oofs += kagg_pred\n",
    "            print ('fold loss: ', log_loss(y[test_ind], fold_pred))            \n",
    "        \n",
    "        print ('iteration OOF score:', log_loss(y, data_oofs/(iter_num+1)))\n",
    "    data_oofs /= iters_total\n",
    "    kagg_oofs /= (iters_total*n_splits)\n",
    "    return data_oofs, kagg_oofs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOF predicitons with LightGBM on TF-IDF transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold loss:  0.439412958122\n",
      "fold loss:  0.446161681867\n",
      "fold loss:  0.438687438293\n",
      "fold loss:  0.440298329498\n",
      "fold loss:  0.434612715998\n",
      "iteration OOF score: 0.439872980869\n",
      "fold loss:  0.43555397353\n",
      "fold loss:  0.458551952053\n",
      "fold loss:  0.43650222986\n",
      "fold loss:  0.42941386793\n",
      "fold loss:  0.454388462144\n",
      "iteration OOF score: 0.435669698071\n",
      "fold loss:  0.429751625751\n",
      "fold loss:  0.452101395873\n",
      "fold loss:  0.451927398582\n",
      "fold loss:  0.439180205335\n",
      "fold loss:  0.437623624326\n",
      "iteration OOF score: 0.43348176463\n",
      "tfidf_oof is done in 15.7 minutes \n",
      "\n",
      "fold loss:  0.580043700385\n",
      "fold loss:  0.576181443632\n",
      "fold loss:  0.577053242991\n",
      "fold loss:  0.575489394876\n",
      "fold loss:  0.57833468949\n",
      "iteration OOF score: 0.577433531727\n",
      "fold loss:  0.573240920204\n",
      "fold loss:  0.591257770222\n",
      "fold loss:  0.576375452266\n",
      "fold loss:  0.578357423251\n",
      "fold loss:  0.569795616364\n",
      "iteration OOF score: 0.573748777017\n",
      "fold loss:  0.575027725913\n",
      "fold loss:  0.575310008771\n",
      "fold loss:  0.574980728799\n",
      "fold loss:  0.579712464421\n",
      "fold loss:  0.583079500806\n",
      "iteration OOF score: 0.572385607784\n",
      "tfidf_pca_oof is done in 2.3 minutes \n",
      "\n",
      "fold loss:  0.441729623125\n",
      "fold loss:  0.442670268185\n",
      "fold loss:  0.449150833135\n",
      "fold loss:  0.452199368568\n",
      "fold loss:  0.436121331111\n",
      "iteration OOF score: 0.444503124447\n",
      "fold loss:  0.444577578698\n",
      "fold loss:  0.460900198576\n",
      "fold loss:  0.439492989287\n",
      "fold loss:  0.427937603195\n",
      "fold loss:  0.441378415934\n",
      "iteration OOF score: 0.43762658028\n",
      "fold loss:  0.438902005469\n",
      "fold loss:  0.436745545475\n",
      "fold loss:  0.447503169981\n",
      "fold loss:  0.441746049426\n",
      "fold loss:  0.456215775744\n",
      "iteration OOF score: 0.435845396729\n",
      "tfidf_and_pca_oof is done in 15.5 minutes \n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/other/data_tfidf_oof.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cfa46c868629>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from sklearn.decomposition import TruncatedSVD\\n\\n# PCA transofmed matrices\\npca = TruncatedSVD(n_components=20)\\npca.fit(data)\\ndata_pca = pca.transform(data)\\nkagg_pca = pca.transform(kagg)\\n\\n# Empty dfs for oof preditions\\ndata_preds = pd.DataFrame()\\nkagg_preds = pd.DataFrame()\\n\\n# LightGBM hyperparameters\\nparams = {\\n        'task': 'train','boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'binary_logloss',\\n        'feature_fraction': 0.95,\\n        'min_data_in_leaf': 10, \\n        'bagging_freq': 3, \\n        'min_gain_to_split': 0, \\n        'lambda_l2': 1, \\n        'learning_rate': 0.075, \\n        'num_leaves': 128, \\n        'bagging_fraction': 0.85}\\n\\nt_start = time.time()\\ndata_preds['tfidf_oof'], kagg_preds['tfidf_oof'] = get_oofs('lgb', data, kagg, ytrain, ids, \\n                                                            params=params)\\nprint ('tfidf_oof is done in {} minutes \\\\n'.format(round((time.time()-t_start)/60,1) ))\\n\\nt_start = time.time()\\ndata_preds['tfidf_pca_oof'], kagg_preds['tfidf_pca_oof'] = get_oofs('lgb', data_pca, kagg_pca, ytrain, ids, \\n                                                                    params=params)\\nprint ('tfidf_pca_oof is done in {} minutes \\\\n'.format(round((time.time()-t_start)/60,1) ))\\n\\nt_start = time.time()\\ndata_pca = efficient_hstack_csr([data, data_pca])\\nkagg_pca = efficient_hstack_csr([kagg, kagg_pca])\\ndata_preds['tfidf_and_pca_oof'], kagg_preds['tfidf_and_pca_oof'] = get_oofs('lgb', data_pca, kagg_pca, ytrain, ids, \\n                                                                            params=params)\\nprint ('tfidf_and_pca_oof is done in {} minutes \\\\n'.format(round((time.time()-t_start)/60,1) ))\\n\\ndata_preds.to_csv('data/other/data_tfidf_oof.csv', index=False)\\nkagg_preds.to_csv('data/other/kagg_tfidf_oof.csv', index=False)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Denis/anaconda/envs/python3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/Denis/anaconda/envs/python3/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Denis/anaconda/envs/python3/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/Denis/anaconda/envs/python3/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1381\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1383\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Denis/anaconda/envs/python3/lib/python3.5/site-packages/pandas/formats/format.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1458\u001b[0m             f = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m   1459\u001b[0m                             \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m                             compression=self.compression)\n\u001b[0m\u001b[1;32m   1461\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Denis/anaconda/envs/python3/lib/python3.5/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path, mode, encoding, compression, memory_map)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/other/data_tfidf_oof.csv'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# PCA transofmed matrices\n",
    "pca = TruncatedSVD(n_components=20)\n",
    "pca.fit(data)\n",
    "data_pca = pca.transform(data)\n",
    "kagg_pca = pca.transform(kagg)\n",
    "\n",
    "# Empty dfs for oof preditions\n",
    "data_preds = pd.DataFrame()\n",
    "kagg_preds = pd.DataFrame()\n",
    "\n",
    "# LightGBM hyperparameters\n",
    "params = {\n",
    "        'task': 'train','boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'binary_logloss',\n",
    "        'feature_fraction': 0.95,\n",
    "        'min_data_in_leaf': 10, \n",
    "        'bagging_freq': 3, \n",
    "        'min_gain_to_split': 0, \n",
    "        'lambda_l2': 1, \n",
    "        'learning_rate': 0.075, \n",
    "        'num_leaves': 128, \n",
    "        'bagging_fraction': 0.85}\n",
    "\n",
    "t_start = time.time()\n",
    "data_preds['tfidf_oof'], kagg_preds['tfidf_oof'] = get_oofs('lgb'\n",
    "                                                            ,data\n",
    "                                                            ,kagg\n",
    "                                                            ,ytrain, \n",
    "                                                            ,ids \n",
    "                                                            ,params=params)\n",
    "print ('tfidf_oof is done in {} minutes \\n'.format(round((time.time()-t_start)/60,1) ))\n",
    "\n",
    "t_start = time.time()\n",
    "data_preds['tfidf_pca_oof'], kagg_preds['tfidf_pca_oof'] = get_oofs('lgb'\n",
    "                                                                    ,data_pca\n",
    "                                                                    ,kagg_pca\n",
    "                                                                    ,ytrain\n",
    "                                                                    ,ids\n",
    "                                                                    ,params=params)\n",
    "print ('tfidf_pca_oof is done in {} minutes \\n'.format(round((time.time()-t_start)/60,1) ))\n",
    "\n",
    "\n",
    "data_preds.to_csv(os.path.join(DATA_PATH, 'data_tfidf_oof.csv'), index=False)\n",
    "kagg_preds.to_csv(os.path.join(DATA_PATH, 'kagg_tfidf_oof.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOF predicitons with factorization machines on TF-IDF transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold loss:  0.659272003041\n",
      "fold loss:  0.664246322973\n",
      "fold loss:  0.664770210204\n",
      "fold loss:  0.67232671434\n",
      "fold loss:  0.667665502412\n",
      "iteration OOF score: 0.665678150633\n",
      "fold loss:  0.66759054704\n",
      "fold loss:  0.663768711036\n",
      "fold loss:  0.667166460172\n",
      "fold loss:  0.667253302442\n",
      "fold loss:  0.662739979867\n",
      "iteration OOF score: 0.665593927196\n",
      "fold loss:  0.65552871689\n",
      "fold loss:  0.665540087076\n",
      "fold loss:  0.663973001235\n",
      "fold loss:  0.667548895915\n",
      "fold loss:  0.66967392152\n",
      "iteration OOF score: 0.665183570757\n",
      "CPU times: user 7.62 s, sys: 1.17 s, total: 8.78 s\n",
      "Wall time: 8.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_preds = pd.DataFrame()\n",
    "kagg_preds = pd.DataFrame()\n",
    "\n",
    "params = {'n_iter':5000, 'init_stdev':5e-3, 'step_size':5e-3, 'l2_reg_w':10, 'l2_reg':10}\n",
    "data_preds['FM_oof'], kagg_preds['FM_oof'] = get_oofs('fastfm', data, kagg, ytrain, ids, \n",
    "                                                      params=params)\n",
    "\n",
    "data_preds.to_csv(os.path.join(DATA_PATH, 'data_fm_oof.csv'), index=False)\n",
    "kagg_preds.to_csv(os.path.join(DATA_PATH, 'kagg_fm_oof.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from keras.engine import Model\n",
    "from keras.engine import Input\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from keras.layers import LSTM, concatenate\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.layers import Embedding\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_FOLDER = '../../data'\n",
    "TRAIN_MODE = True\n",
    "NUM_EPOCHS = 10\n",
    "NUM_FILTERS = 100\n",
    "KERNEL_SIZE = 3\n",
    "OUTPUT_SIZE = 128\n",
    "MAX_FEATURES = 200000\n",
    "EMBEDDING_SIZE = 300\n",
    "DROPOUT_PROB = 0.5\n",
    "MAX_LEN = 20\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE = 3\n",
    "WEIGHTS_NAME = 'weights_convnet.h5'\n",
    "COMPILED_MODEL_NAME = 'compiled_convnet_model.h5'\n",
    "TRAIN_OUTPUT_PROB_NAME = '../../data/train_convnet_proba.pkl'\n",
    "GLOVE_VECTORDS_PATH = '../../data/glove.840B.300d.txt'\n",
    "INPUT_TRAIN_FILE = '../../data/train.csv'\n",
    "INPUT_TEST_FILE = '../../data/test.csv'\n",
    "TEST_OUTPUT_PROB_NAME = '../../data/test_convnet_proba.pkl'\n",
    "BASE_DIR = '../../data/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin.gz'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'train.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'test.csv'\n",
    "\n",
    "# print('Processing text dataset')\n",
    "#\n",
    "#\n",
    "# # The function \"text_to_wordlist\" is from\n",
    "# # https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "# def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "#     # Clean the text, with the option to remove stopwords and to stem words.\n",
    "#\n",
    "#     # Convert words to lower case and split them\n",
    "#     text = text.lower().split()\n",
    "#\n",
    "#     # Optionally, remove stop words\n",
    "#     if remove_stopwords:\n",
    "#         stops = set(stopwords.words(\"english\"))\n",
    "#         text = [w for w in text if not w in stops]\n",
    "#\n",
    "#     text = \" \".join(text)\n",
    "#\n",
    "#     # Clean the text\n",
    "#     text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "#     text = re.sub(r\"what's\", \"what is \", text)\n",
    "#     text = re.sub(r\"\\'s\", \" \", text)\n",
    "#     text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "#     text = re.sub(r\"can't\", \"cannot \", text)\n",
    "#     text = re.sub(r\"n't\", \" not \", text)\n",
    "#     text = re.sub(r\"i'm\", \"i am \", text)\n",
    "#     text = re.sub(r\"\\'re\", \" are \", text)\n",
    "#     text = re.sub(r\"\\'d\", \" would \", text)\n",
    "#     text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "#     text = re.sub(r\",\", \" \", text)\n",
    "#     text = re.sub(r\"\\.\", \" \", text)\n",
    "#     text = re.sub(r\"!\", \" ! \", text)\n",
    "#     text = re.sub(r\"\\/\", \" \", text)\n",
    "#     text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "#     text = re.sub(r\"\\+\", \" + \", text)\n",
    "#     text = re.sub(r\"\\-\", \" - \", text)\n",
    "#     text = re.sub(r\"\\=\", \" = \", text)\n",
    "#     text = re.sub(r\"'\", \" \", text)\n",
    "#     text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "#     text = re.sub(r\":\", \" : \", text)\n",
    "#     text = re.sub(r\" e g \", \" eg \", text)\n",
    "#     text = re.sub(r\" b g \", \" bg \", text)\n",
    "#     text = re.sub(r\" u s \", \" american \", text)\n",
    "#     text = re.sub(r\"\\0s\", \"0\", text)\n",
    "#     text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "#     text = re.sub(r\"e - mail\", \"email\", text)\n",
    "#     text = re.sub(r\"j k\", \"jk\", text)\n",
    "#     text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "#\n",
    "#     # Optionally, shorten words to their stems\n",
    "#     if stem_words:\n",
    "#         text = text.split()\n",
    "#         stemmer = SnowballStemmer('english')\n",
    "#         stemmed_words = [stemmer.stem(word) for word in text]\n",
    "#         text = \" \".join(stemmed_words)\n",
    "#\n",
    "#     # Return a list of words\n",
    "#     return (text)\n",
    "#\n",
    "#\n",
    "# texts_1 = []\n",
    "# texts_2 = []\n",
    "# YTrain = []\n",
    "# with codecs.open(TRAIN_DATA_FILE) as f:\n",
    "#     reader = csv.reader(f, delimiter=',')\n",
    "#     header = next(reader)\n",
    "#     for values in reader:\n",
    "#         texts_1.append(text_to_wordlist(values[3]))\n",
    "#         texts_2.append(text_to_wordlist(values[4]))\n",
    "#         YTrain.append(int(values[5]))\n",
    "# print('Found %s texts in train.csv' % len(texts_1))\n",
    "#\n",
    "# test_texts_1 = []\n",
    "# test_texts_2 = []\n",
    "# test_ids = []\n",
    "# with codecs.open(TEST_DATA_FILE) as f:\n",
    "#     reader = csv.reader(f, delimiter=',')\n",
    "#     header = next(reader)\n",
    "#     for values in reader:\n",
    "#         test_texts_1.append(text_to_wordlist(values[1]))\n",
    "#         test_texts_2.append(text_to_wordlist(values[2]))\n",
    "#         test_ids.append(values[0])\n",
    "# print('Found %s texts in test.csv' % len(test_texts_1))\n",
    "#\n",
    "# tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "# tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "#\n",
    "# sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "# sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "# test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "# test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "#\n",
    "# word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens' % len(word_index))\n",
    "#\n",
    "# x1_train = pad_sequences(sequences_1, maxlen=MAX_LEN)\n",
    "# x2_train = pad_sequences(sequences_2, maxlen=MAX_LEN)\n",
    "# YTrain = np.array(YTrain)\n",
    "# print('Shape of data tensor:', x1_train.shape)\n",
    "# print('Shape of label tensor:', YTrain.shape)\n",
    "#\n",
    "# x1_test = pad_sequences(test_sequences_1, maxlen=MAX_LEN)\n",
    "# x2_test = pad_sequences(test_sequences_2, maxlen=MAX_LEN)\n",
    "# test_ids = np.array(test_ids)\n",
    "#\n",
    "# nb_words = min(MAX_FEATURES, len(word_index)) + 1\n",
    "#\n",
    "# with open('dump_sequences.pkl', 'wb') as f:\n",
    "#     pickle.dump([x1_train, x2_train, x1_test, x2_test, nb_words], f)\n",
    "#\n",
    "# print('Preparing embedding matrix')\n",
    "# word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "# print('Found %s word vectors of word2vec' % len(word2vec.vocab))\n",
    "# embedding_matrix = np.zeros((nb_words, EMBEDDING_SIZE))\n",
    "# for word, i in word_index.items():\n",
    "#     if word in word2vec.vocab:\n",
    "#         embedding_matrix[i] = word2vec.word_vec(word)\n",
    "# print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "# np.save('embedding_matrix', embedding_matrix)\n",
    "\n",
    "import cPickle as pickle\n",
    "with open('dump_sequences.pkl', 'rb') as f:\n",
    "    x1_train, x2_train, x1_test, x2_test, nb_words = pickle.load(f)\n",
    "embedding_matrix = np.load('embedding_matrix.npy')\n",
    "YTrain = pd.read_csv(INPUT_FOLDER + '/train.csv', usecols=['is_duplicate']).values.ravel()\n",
    "\n",
    "class KerasConvnet:\n",
    "    def __init__(self, use_weights=None):\n",
    "\n",
    "        num_lstm = 267\n",
    "        num_dense = 117\n",
    "        rate_drop_lstm = 0.25\n",
    "        rate_drop_dense = 0.15\n",
    "\n",
    "        act = 'relu'\n",
    "\n",
    "        embedding_layer = Embedding(nb_words,\n",
    "                                    EMBEDDING_SIZE,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_LEN,\n",
    "                                    trainable=False)\n",
    "        lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "        sequence_1_input = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "        embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "        x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "        sequence_2_input = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "        embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "        y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "        merged = concatenate([x1, y1])\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        merged = Dense(num_dense, activation=act)(merged)\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "        preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "        model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n",
    "\n",
    "        if use_weights is not None:\n",
    "            model.load_weights(use_weights)\n",
    "            \n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='nadam',\n",
    "                      metrics=['acc'])\n",
    "        model.summary()\n",
    "\n",
    "        self.model = model\n",
    "        self.earlystop = EarlyStopping(monitor='val_loss', patience=3, verbose=0)\n",
    "        self.checkpoint = ModelCheckpoint(WEIGHTS_NAME, monitor='val_loss', save_best_only=True, verbose=2)\n",
    "        self.class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "\n",
    "    def fit(self, train1, train2, y_train):\n",
    "        print 'Fitting ... '\n",
    "        input_arrays = [train1, train2]\n",
    "        self.model.fit(input_arrays, y_train, batch_size=BATCH_SIZE,\n",
    "                       epochs=NUM_EPOCHS, validation_split=0.1, shuffle=True,\n",
    "                       callbacks=[self.earlystop, self.checkpoint])\n",
    "\n",
    "    def predict(self, test1, test2):\n",
    "        input_arrays = [test1, test2]\n",
    "        preds = self.model.predict(input_arrays, batch_size=BATCH_SIZE, verbose=1)\n",
    "        for _ in range(4):\n",
    "            preds += self.model.predict(input_arrays, batch_size=BATCH_SIZE, verbose=1)\n",
    "        preds /= 5.\n",
    "\n",
    "        return preds\n",
    "\n",
    "print x1_train.shape, x2_train.shape\n",
    "print x1_test.shape, x2_test.shape\n",
    "print nb_words\n",
    "\n",
    "N_FOLDS = 3\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS).split(np.zeros((x1_train.shape[0], 1)), YTrain)\n",
    "splits = list(skf)\n",
    "\n",
    "print \"Creating train and test sets for blending.\"\n",
    "\n",
    "preds_train = np.zeros((x1_train.shape[0],))\n",
    "loglosses = []\n",
    "\n",
    "preds_test_j = np.zeros((x1_test.shape[0], N_FOLDS))\n",
    "for fold_id, (train_indexes, predict_indexes) in enumerate(splits):\n",
    "    print \"Fold\", fold_id\n",
    "\n",
    "    clf = KerasConvnet()\n",
    "    clf.fit(x1_train[train_indexes], x2_train[train_indexes], YTrain[train_indexes])\n",
    "\n",
    "    y_pred = clf.predict(x1_train[predict_indexes], x2_train[predict_indexes])\n",
    "    preds_train[predict_indexes] = y_pred\n",
    "\n",
    "    lloss = log_loss(YTrain[predict_indexes], y_pred)\n",
    "    loglosses.append(lloss)\n",
    "    print 'LogLoss: ', lloss\n",
    "\n",
    "    # Predict on entire test set\n",
    "    preds_test_j[:, fold_id] = clf.predict(x1_test, x2_test).ravel()\n",
    "\n",
    "print \"Out of fold logloss-es:\\n\", loglosses\n",
    "\n",
    "# Save OOF predictions as features\n",
    "pd.DataFrame(preds_train, columns=['siamese_oof']).to_csv(INPUT_FOLDER + '/features/siamese_oof_train.csv')\n",
    "\n",
    "# Save mean of predictions for test\n",
    "preds_test = preds_test_j.mean(1)\n",
    "pd.DataFrame(preds_test, columns=['siamese_oof']).to_csv(INPUT_FOLDER + '/features/siamese_oof_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
