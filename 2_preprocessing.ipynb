{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-poster')\n",
    "sns.set_palette('Set1', 10, desat=0.75)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/'\n",
    "USE_SPELLCHECK = False\n",
    "NUM_CORES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404290 404288 290457 299174\n",
      "2345796 2345790 2211008 2227399\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))#[:10000]\n",
    "data.columns = ['id', 'qid1', 'qid2', 'q1', 'q2', 'target']\n",
    "print (data.shape[0], data.dropna().shape[0], data.q1.nunique(), data.q2.nunique())\n",
    "data.fillna('xxx', inplace=True)\n",
    "\n",
    "kagg = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))#[:20000]\n",
    "kagg.columns = ['test_id', 'q1', 'q2']\n",
    "print (kagg.shape[0], kagg.dropna().shape[0], kagg.q1.nunique(), kagg.q2.nunique())\n",
    "kagg.fillna('xxx', inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "if USE_SPELLCHECK:\n",
    "    with open(os.path.join('spellchecking', 'gc_train.pkl'), 'rb') as gc_dict:\n",
    "        gc_train = pickle.load(gc_dict)\n",
    "    data.q1 = data.q1.apply(lambda q: gc_train.get(q, q))\n",
    "    data.q2 = data.q2.apply(lambda q: gc_train.get(q, q))\n",
    "    \n",
    "    with open(os.path.join('spellchecking', 'gc_test.pkl'), 'rb') as gc_dict:\n",
    "        gc_test = pickle.load(gc_dict)\n",
    "    kagg.q1 = kagg.q1.apply(lambda q: gc_test.get(q, q))\n",
    "    kagg.q2 = kagg.q2.apply(lambda q: gc_test.get(q, q))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "source": [
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "class SpellingReplacer(object):\n",
    "    def __init__(self, dict_name='en', max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = max_dist\n",
    "\n",
    "    def replace(self, word):\n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "\n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "            return suggestions[0]\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "def correct_question(q):\n",
    "    new_words = []\n",
    "    words = q.split(' ')\n",
    "    for w in words:\n",
    "        if w!='' and w!='xxx':\n",
    "            if w not in enchant_spelling :\n",
    "                enchant_spelling[w] = replacer.replace(w)\n",
    "            new_words.append(enchant_spelling[w])\n",
    "    new_q = ' '.join(new_words)\n",
    "    if new_q!=q:\n",
    "        print(q)\n",
    "        print(new_q)\n",
    "        print ()\n",
    "    return new_q\n",
    "\n",
    "replacer = SpellingReplacer()\n",
    "\n",
    "with open(os.path.join('spellchecking', 'enchant_spelling.pkl'), 'rb') as spelling_dict:\n",
    "    enchant_spelling = pickle.load(spelling_dict)\n",
    "\n",
    "print (correct_question('hwo to buildl a   tableg quikly'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "source": [
    "%%time\n",
    "def correct_data(data):\n",
    "    for col in ['q1', 'q2']:\n",
    "        data[col+'_corr'] = [correct_question(q) for q in data[col]]\n",
    "        print (col, 'done')\n",
    "    data.replace('', 'xxx', inplace=True)\n",
    "    return data\n",
    "\n",
    "#data = correct_data(data)\n",
    "#kagg = correct_data(kagg)\n",
    "\n",
    "print (len(set(list(enchant_spelling.values()))), len(set(list(enchant_spelling.keys()))))\n",
    "with open(os.path.join('spellchecking', 'enchant_spelling.pkl'), 'wb') as spelling_dict:\n",
    "    pickle.dump(enchant_spelling, spelling_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 53s, sys: 1.74 s, total: 2min 55s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def sentence_lower_regex(text):  \n",
    "    \"\"\"\n",
    "    input: text string\n",
    "    output: string with no capital letters and some common English abbrv replaced\n",
    "    \"\"\"\n",
    "    text = text.split(' ')\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = [w.lower() for w in text]\n",
    "\n",
    "    # Clean the text\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"\\'s\", \" 's \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \" cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\";\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def data_lower_regex(data):\n",
    "    data.q1 = [sentence_lower_regex(q) for q in data.q1]\n",
    "    data.q2 = [sentence_lower_regex(q) for q in data.q2]\n",
    "    data.replace('', 'xxx', inplace=True)\n",
    "    return data\n",
    "\n",
    "data = data_lower_regex(data)\n",
    "kagg = data_lower_regex(kagg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import multiprocessing as mp\n",
    "\n",
    "def apply_parallel(df, my_func):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        df: pandas DataFrame or pandas Series\n",
    "        my_func: custom function which will be apllied to df. Must accept pandas DataFrame or Series as input.\n",
    "    Output: concatenated results of function application on DataFrame. Either pandas Series or pandas DataFrame.\n",
    "    \n",
    "    df is splitted by the number of cores and function applied to each part independetly.\n",
    "    Results are concatenated and returned\n",
    "    \"\"\"\n",
    "    df_splitted = np.array_split(df, NUM_CORES)\n",
    "    pool = mp.Pool(NUM_CORES)\n",
    "    result = pd.concat(pool.map(my_func, df_splitted))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return result\n",
    "\n",
    "def lemmatize_col(pd_series):\n",
    "    \"\"\"\n",
    "    Input: pandas Series with text inside\n",
    "    Output: pandas Series with text lemmatized with WordNetLemmatizer\n",
    "    Word order is unchanged\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    def lemm_text(text):\n",
    "        text = ' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])\n",
    "        return text\n",
    "    return pd_series.astype(str).apply(lemm_text)\n",
    "\n",
    "def stem_col(pd_series):\n",
    "    \"\"\"\n",
    "    Input: pandas Series with text inside\n",
    "    Output: pandas Series with text stemmed with PorterStemmer\n",
    "    Word order is unchanged\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    def stem_text(text):\n",
    "        text = \" \".join([stemmer.stem(word) for word in text.split(' ')])\n",
    "        return text\n",
    "    return pd_series.astype(str).apply(stem_text)\n",
    "\n",
    "\n",
    "# Identify stopwords for remove_stopwords_col function\n",
    "stop_words_set = set(nltk.corpus.stopwords.words(\"english\")) \\\n",
    "                -set(['nor', 'both', 'same', 'against', 'between', 'because', 'not', 'won', 'before', 'doesn',\n",
    "                     'most', 'shouldn', 'mustn', 'needn', 'wouldn', 'couldn', 'mightn', 'wasn', 'aren', 'isn',\n",
    "                     'why', 'were', 'no', 'hadn', 'didn', 'weren'])\n",
    "\n",
    "def remove_stopwords_col(pd_series):\n",
    "    \"\"\"\n",
    "    input: pandas Series with text inside\n",
    "    output: pandas Series with text with stopwords remove\n",
    "    Word order is unchanged\n",
    "    \"\"\"\n",
    "    def remove_stops_text(text):\n",
    "        text = ' '.join([word for word in text.split(' ') if not word in stop_words_set])\n",
    "        return text\n",
    "    return pd_series.astype(str).apply(remove_stops_text)\n",
    "\n",
    "def get_tags(pd_series):\n",
    "    \"\"\"\n",
    "    input: pd Series with question texts. Must not contain Nulls\n",
    "    output: pd Series, with nltk part-of-speech tags\n",
    "    \"\"\"\n",
    "    return pd_series.apply(lambda Q: ' '.join([word_tag[1] for word_tag in nltk.pos_tag(Q.split(' '))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 s, sys: 15.3 s, total: 35.6 s\n",
      "Wall time: 21min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create new columns in datasets with postfixes corresponding to data processing type\n",
    "# E.g. column 'q1' after stemming will be named 'q1_stem'\n",
    "# Also renames columns 'q1' and 'q2' to 'q1_src' and 'q2_src'\n",
    "\n",
    "for col in ['q1', 'q2']:\n",
    "    data[col+'_stem'] = apply_parallel(data[col], stem_col)\n",
    "    kagg[col+'_stem'] = apply_parallel(kagg[col], stem_col)\n",
    "    \n",
    "    data[col+'_nostops'] = apply_parallel(data[col], remove_stopwords_col)\n",
    "    kagg[col+'_nostops'] = apply_parallel(kagg[col], remove_stopwords_col)\n",
    "    \n",
    "    data[col+'_tags'] = apply_parallel(data[col], get_tags)\n",
    "    kagg[col+'_tags'] = apply_parallel(kagg[col], get_tags)\n",
    "\n",
    "    data = data.rename(columns={col: col+'_src'})\n",
    "    kagg = kagg.rename(columns={col: col+'_src'})\n",
    "\n",
    "# replace empty strings with xxx. Empty strings may oocur if there were only stopwords in question\n",
    "data.replace('', 'xxx', inplace=True)\n",
    "kagg.replace('', 'xxx', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 23s, sys: 26.9 s, total: 9min 50s\n",
      "Wall time: 10min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def text_intersect_col(df):\n",
    "    \"\"\"\n",
    "    Input: pandas DataFrame. Must have two columns, corresponding to two questions\n",
    "    Output: pandas Series, consisting of words present in both questions\n",
    "    Word order is maintained from the first question.\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    Q1 = df[cols[0]]\n",
    "    Q2 = df[cols[1]]\n",
    "    \n",
    "    def text_intersect(t1, t2):\n",
    "        intersection = []\n",
    "        t1_words = dict((word,0) for word in t1.split(' '))\n",
    "        for word in t2.split(' '):\n",
    "            if word in t1_words:\n",
    "                intersection.append(word)\n",
    "        return ' '.join(intersection)\n",
    "    \n",
    "    df['inter'] = [text_intersect(q1,q2) for q1,q2 in zip(Q1, Q2)]\n",
    "    return df['inter']\n",
    "\n",
    "def extra_words(q1, q2, inter):\n",
    "    \"\"\"\n",
    "    input: three strings: two questions and shared words.\n",
    "    output: string, containing all words, that are present only in question1 and not in question2 and vice versa.\n",
    "    In terms os sets output = (q1+q2) - (q1*q2) \n",
    "    \n",
    "    ! Output is not ordered ! \n",
    "    \"\"\"\n",
    "    return ' '.join(list(set(q1.split(' ') + q2.split(' ')) - set(inter.split(' '))))\n",
    "\n",
    "    \n",
    "# create a column for intersection for each processing type\n",
    "postfixes = ['_src', '_stem', '_nostops', '_tags']\n",
    "for p in postfixes:\n",
    "    data['inter'+p] = apply_parallel(data[['q1'+p, 'q2'+p]], text_intersect_col)\n",
    "    kagg['inter'+p] = apply_parallel(kagg[['q1'+p, 'q2'+p]], text_intersect_col)\n",
    "    \n",
    "    data['extra'+p] = data.apply(lambda row: extra_words(row['q1'+p], row['q2'+p], row['inter'+p]), axis=1)\n",
    "    kagg['extra'+p] = kagg.apply(lambda row: extra_words(row['q1'+p], row['q2'+p], row['inter'+p]), axis=1)\n",
    "    \n",
    "data.replace('', 'xxx', inplace=True)\n",
    "kagg.replace('', 'xxx', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv(os.path.join(DATA_PATH, 'train_preprocessed.csv'), sep=';', index=False)\n",
    "kagg.to_csv(os.path.join(DATA_PATH, 'test_preprocessed.csv' ), sep=';', index=False)\n",
    "data[['target']].to_csv(os.path.join(DATA_PATH, 'target.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Construciton of questions and graphs ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add question ids to the test data. As can be seen from train data, ids are constructed consequentially. First question is assigned to id #1, second to id #2 and so on. If both questions from pair haven't been seen in data, the quesiotn in th field q1 recieves lower id, than in q2 field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 46s, sys: 2.92 s, total: 4min 48s\n",
      "Wall time: 4min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question_id_dict = dict()\n",
    "current_id = 1\n",
    "\n",
    "for _,row in pd.concat([data, kagg], axis=0).iterrows():\n",
    "    q1 = row.q1\n",
    "    q2 = row.q2\n",
    "    \n",
    "    if q1 not in question_id_dict:\n",
    "        question_id_dict[q1] = current_id\n",
    "        current_id+=1\n",
    "    if q2 not in question_id_dict:\n",
    "        question_id_dict[q2] = current_id\n",
    "        current_id+=1\n",
    "        \n",
    "data['qid1'] = data.q1.apply(lambda q: question_id_dict[q])\n",
    "data['qid2'] = data.q2.apply(lambda q: question_id_dict[q])\n",
    "\n",
    "kagg['qid1'] = kagg.q1.apply(lambda q: question_id_dict[q])\n",
    "kagg['qid2'] = kagg.q2.apply(lambda q: question_id_dict[q])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add graph ids to train data, since later we will split validation data by graph ids (all questions from the same graph must be in the same fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249770\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(data['qid1'])\n",
    "G.add_nodes_from(data['qid2'])\n",
    "edges = list(data[['qid1', 'qid2']].to_records(index=False))\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "graph_dict = dict()\n",
    "graph_id = 1\n",
    "for i in range(1,len(G)+1):\n",
    "    graph = G[i]\n",
    "    if i not in graph_dict:\n",
    "        graph_dict[i] = graph_id\n",
    "        \n",
    "    for qid in graph:\n",
    "        if qid not in graph_dict:\n",
    "            graph_dict[qid] = graph_id\n",
    "    graph_id+=1\n",
    "    \n",
    "print (len(set(graph_dict.values())))  \n",
    "data['graph_id'] = data.qid1.apply(lambda x: graph_dict[x])\n",
    "\n",
    "data[['qid1', 'qid2', 'graph_id']].to_csv(os.path.join(DATA_PATH, 'train_ids.csv'), index=False)\n",
    "kagg[['qid1', 'qid2']]            .to_csv(os.path.join(DATA_PATH, 'test_ids.csv' ), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
