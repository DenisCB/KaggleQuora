{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-poster')\n",
    "sns.set_palette('Set1', 10, desat=0.75)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/'\n",
    "postfixes = ['_src', '_stem', '_nostops']  \n",
    "NUM_CORES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "def apply_parallel(df, my_func):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        df: pandas DataFrame or pandas Series\n",
    "        my_func: custom function which will be apllied to df. Must accept pandas DataFrame or Series as input.\n",
    "    Output: concatenated results of function application on DataFrame. Either pandas Series or pandas DataFrame.\n",
    "    \n",
    "    df is splitted by the number of cores and function applied to each part independetly.\n",
    "    Results are concatenated and returned\n",
    "    \"\"\"\n",
    "    df_splitted = np.array_split(df, NUM_CORES)\n",
    "    pool = mp.Pool(NUM_CORES)\n",
    "    result = pd.concat(pool.map(my_func, df_splitted))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(DATA_PATH, 'train_preprocessed.csv'), sep=';')\n",
    "data.fillna('xxx', inplace=True)\n",
    "\n",
    "kagg = pd.read_csv(os.path.join(DATA_PATH, 'test_preprocessed.csv'), sep=';')\n",
    "kagg.fillna('xxx', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF cosine distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_parallel_sparse(DF, my_func):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        DF: scipy sparse matrix\n",
    "        my_func: custom function which will be apllied to DF. Must accept scipy sparse matrix and return a list.\n",
    "    Output: concatenated results of function application on DataFrame. Either pandas Series or pandas DataFrame.\n",
    "    \n",
    "    DF is splitted by the number of cores and function applied to each part independetly.\n",
    "    Results are combined in one list and returned\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get split indices for DF to split on batches with (total_rows/NUM_CORES) rows each\n",
    "    total_rows = DF.shape[0]\n",
    "    split_indices = []\n",
    "    for i in range(NUM_CORES):\n",
    "        split_on = (int(total_rows/NUM_CORES)*i)\n",
    "        split_indices.append(split_on)\n",
    "    split_indices.append(total_rows)\n",
    "    \n",
    "    # Replace input matrix with list of NUM_CORES matricies\n",
    "    DF = [DF[split_indices[i]:split_indices[i+1]] for i in range(NUM_CORES)]\n",
    "    \n",
    "    # Apply my_func to each element of a DF list (sparse matricies) and construct final results list\n",
    "    res = []\n",
    "    pool = mp.Pool(NUM_CORES)\n",
    "    for part_res in pool.map(my_func, DF):\n",
    "        res+=part_res\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return res\n",
    "\n",
    "def efficient_hstack_csr(matrices, batch_size=1e5):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        matrices: list of matrices to be hstacked. All must have same row number. \n",
    "            Acceptable formats: csr matrix, coo matric, pandas DataFrame, numpy array.\n",
    "        batch_size: int, number of rows to hstack per batch. The lower the batch, the lower the memory footprint.\n",
    "        Recommended batch size 10-200K.\n",
    "    Output: essentially just sparse.hstack(csr_matrices, format='csr'), but much more efficient\n",
    "    \n",
    "    Hstack operation works ok only on csc matrices or small csr matrices, so we can't apply it on tfidf ouputs as is. \n",
    "    Therefore we will hstack small batches with each other and after that vstack all resulting batches. Vstack works\n",
    "    the best with csr format, as we need. \n",
    "    \"\"\"\n",
    "    \n",
    "    batches = []\n",
    "    batch_size = int(batch_size) # convert float to int in case if passed like 1e5\n",
    "    for i in range(0, matrices[0].shape[0], batch_size):\n",
    "        lower_bound = i\n",
    "        upper_bound = min(i+batch_size, matrices[0].shape[0])\n",
    "        batches.append(sparse.hstack([matrix[lower_bound:upper_bound] for matrix in matrices]\n",
    "                                     , format='csr'))\n",
    "    \n",
    "    return sparse.vstack(batches, format='csr')\n",
    "\n",
    "\n",
    "def cosine_sim(DF):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        DF: scipy sparse matrix. Must be constracted preliminarily from two sparse matricies with the \n",
    "        same number of columns. Each of sparse matrices - tfidf sparse representation of one of the questions.\n",
    "    Output: list of cosine similarities between two halves of DF\n",
    "    \n",
    "    Usage example:\n",
    "    tf_q1 = tfidf.transform(train['q1_src'])\n",
    "    tf_q2 = tfidf.transform(train['q2_src'])\n",
    "    train['cosine_similarities'] = cosine_sim(efficient_hstack_csr(tf_q1, tf_q2, 1e5))\n",
    "    \"\"\"\n",
    "    \n",
    "    split_halves_on = int(DF.shape[1]/2) # Get number of columns in each part\n",
    "    df1 = DF[:, :split_halves_on]\n",
    "    df2 = DF[:, split_halves_on:]\n",
    "    return [cosine_similarity(df1[i], df2[i])[0][0] for i in range(df1.shape[0])] \n",
    "\n",
    "def get_cosine_similarity_features(train, test, p, token, min_n, max_n):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        train: pd DataFrame with questions ins format 'q1'+postfix and 'q2'+postfix. Tf-idf is trained on \n",
    "        corpus from this DataFrame\n",
    "        test: pd DataFrame with questions ins format 'q1'+postfix and 'q2'+postfix\n",
    "        p: postfix ('_src' or '_stem' or '_nostops')\n",
    "        token: 'word' or 'char'. Analyzer in Tf-idf is set to token\n",
    "        min_n, max_n: integers, used for ngram_range in Tf-idf\n",
    "    Output: train and test pd DataFrames with cosine similarities columns added\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct a corpus from all questions from train dataset and fit tfidf to it.\n",
    "    corpus = pd.DataFrame(train['q1'+p].tolist() + train['q2'+p].tolist(), columns=['full_text'])\n",
    "    tfidf = TfidfVectorizer(max_df=0.8, min_df=3, analyzer=token, ngram_range=(min_n, max_n)).fit(corpus.full_text) \n",
    "    del corpus\n",
    "    \n",
    "    # Get tfidf vectors for each question, intersection and resudial\n",
    "    tf_q1 = tfidf.transform(train['q1'   +p])\n",
    "    tf_q2 = tfidf.transform(train['q2'   +p])\n",
    "    tf_in = tfidf.transform(train['inter'+p])\n",
    "    tf_ex = tfidf.transform(train['extra'+p])\n",
    "    \n",
    "    # Calculate cosine distances in parallel\n",
    "    train['DF_cos_q1_q2_'+token+str(min_n)+str(max_n)+p] = apply_parallel_sparse(efficient_hstack_csr([tf_q1, tf_q2]), cosine_sim)\n",
    "    train['DF_cos_q1_in_'+token+str(min_n)+str(max_n)+p] = apply_parallel_sparse(efficient_hstack_csr([tf_q1, tf_in]), cosine_sim)\n",
    "    train['DF_cos_q1_ex_'+token+str(min_n)+str(max_n)+p] = apply_parallel_sparse(efficient_hstack_csr([tf_q1, tf_ex]), cosine_sim)\n",
    "    train['DF_cos_q2_in_'+token+str(min_n)+str(max_n)+p] = apply_parallel_sparse(efficient_hstack_csr([tf_q2, tf_in]), cosine_sim)\n",
    "    train['DF_cos_q2_ex_'+token+str(min_n)+str(max_n)+p] = apply_parallel_sparse(efficient_hstack_csr([tf_q2, tf_ex]), cosine_sim)\n",
    "    \n",
    "    # Get tfidf vectors for each question, intersection and resudial for test\n",
    "    tf_q1  = tfidf.transform(test ['q1'   +p])\n",
    "    tf_q2  = tfidf.transform(test ['q2'   +p])\n",
    "    tf_in  = tfidf.transform(test ['inter'+p])\n",
    "    tf_ex  = tfidf.transform(test ['extra'+p])\n",
    "    \n",
    "    # Calculate cosine distances in parallel for test\n",
    "    test['DF_cos_q1_q2_'+token+str(min_n)+str(max_n)+p] = apply_parallel_sparse(efficient_hstack_csr([tf_q1, tf_q2]), cosine_sim)\n",
    "    test['DF_cos_q1_in_'+token+str(min_n)+str(max_n)+p] = apply_parallel_sparse(efficient_hstack_csr([tf_q1, tf_in]), cosine_sim)\n",
    "    test['DF_cos_q1_ex_'+token+str(min_n)+str(max_n)+p] = apply_parallel_sparse(efficient_hstack_csr([tf_q1, tf_ex]), cosine_sim)\n",
    "    test['DF_cos_q2_in_'+token+str(min_n)+str(max_n)+p] = apply_parallel_sparse(efficient_hstack_csr([tf_q2, tf_in]), cosine_sim)\n",
    "    test['DF_cos_q2_ex_'+token+str(min_n)+str(max_n)+p] = apply_parallel_sparse(efficient_hstack_csr([tf_q2, tf_ex]), cosine_sim)\n",
    "    \n",
    "    return train, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_stem word 1 1\n",
      "Done in 0.6 minutes \n",
      "\n",
      "_stem char 3 3\n",
      "Done in 0.7 minutes \n",
      "\n",
      "_nostops word 2 2\n",
      "Done in 0.5 minutes \n",
      "\n",
      "CPU times: user 15 s, sys: 2.98 s, total: 18 s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p_list =        ['_stem', '_stem', '_nostops']\n",
    "\n",
    "token_list    = ['word',  'char',  'word'] # 'word' or 'char'\n",
    "min_ngram_list =[  1,        3,      2]    # lower ngram range limit\n",
    "max_ngram_list =[  1,        3,      2]    # upper ngram range limit\n",
    "\n",
    "for p, token, min_ngram, max_ngram in zip(p_list, analyzer_list, min_ngram_list, max_ngram_list):\n",
    "    t_start = time.time()\n",
    "    print (p, token, min_ngram, max_ngram,)\n",
    "    \n",
    "    data, kagg = get_cosine_similarity_features(train=data, test=kagg,\n",
    "                                p=p, token=token, min_n=min_ngram, max_n=max_ngram)\n",
    "  \n",
    "    print ('Done in {} minutes \\n'.format(round((time.time()-t_start)/60,1) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import difflib, distance, datasketch\n",
    "from simhash import Simhash\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def minhash_sim_2gram (df):\n",
    "    \"\"\"\n",
    "    Input: pandas DataFrame. Must have two columns, corresponding to two questions\n",
    "    Output: pandas Series, consisting of jaccard distances on minhashed calculated on 2-words ngrams\n",
    "    \n",
    "    More information on MinHash similarity:\n",
    "        Russian: https://habrahabr.ru/post/115147/\n",
    "        English: https://en.wikipedia.org/wiki/MinHash\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    Q1 = df[cols[0]]\n",
    "    Q2 = df[cols[1]]\n",
    "    \n",
    "    def minhash_sim(q1,q2):    \n",
    "        m1 = datasketch.MinHash(num_perm=256)\n",
    "        for d in nltk.ngrams(q1.split(' '), 2):\n",
    "            m1.update(\"\".join(d).encode('utf-8'))\n",
    "\n",
    "        m2 = datasketch.MinHash(num_perm=256)\n",
    "        for d in nltk.ngrams(q2.split(' '), 2):\n",
    "            m2.update(\"\".join(d).encode('utf-8'))\n",
    "\n",
    "        wmh1 = datasketch.WeightedMinHash(0, m1.hashvalues)\n",
    "        wmh2 = datasketch.WeightedMinHash(0, m2.hashvalues)\n",
    "        return wmh1.jaccard(wmh2)\n",
    "    \n",
    "    df['result'] = [minhash_sim(q1,q2) for q1,q2 in zip(Q1, Q2)]\n",
    "    return df['result']\n",
    "\n",
    "def minhash_sim_3gram (df):\n",
    "    \"\"\"\n",
    "    Input: pandas DataFrame. Must have two columns, corresponding to two questions\n",
    "    Output: pandas Series, consisting of jaccard distances on minhashed calculated on 3-words ngrams\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    Q1 = df[cols[0]]\n",
    "    Q2 = df[cols[1]]\n",
    "    \n",
    "    def minhash_sim(q1,q2):    \n",
    "        m1 = datasketch.MinHash(num_perm=256)\n",
    "        for d in nltk.ngrams(q1.split(' '), 3):\n",
    "            m1.update(\"\".join(d).encode('utf-8'))\n",
    "\n",
    "        m2 = datasketch.MinHash(num_perm=256)\n",
    "        for d in nltk.ngrams(q2.split(' '), 3):\n",
    "            m2.update(\"\".join(d).encode('utf-8'))\n",
    "\n",
    "        wmh1 = datasketch.WeightedMinHash(0, m1.hashvalues)\n",
    "        wmh2 = datasketch.WeightedMinHash(0, m2.hashvalues)\n",
    "        return wmh1.jaccard(wmh2)\n",
    "    \n",
    "    df['result'] = [minhash_sim(q1,q2) for q1,q2 in zip(Q1, Q2)]\n",
    "    return df['result']\n",
    "\n",
    "def fuzzy_features(df):\n",
    "    \"\"\"\n",
    "    Input: pandas DataFrame. Must have two columns, corresponding to two questions. Format needed: q1+postfix \n",
    "    and q2+postfix, eg: q1_src, q2_src.\n",
    "    Output: pandas DataFrame, containing 7 columns for diffirenet distance features from Fuzzywuzzy module\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    p = cols[0][2:] # _src or _nostops\n",
    "    \n",
    "    Q1 = cols[0]\n",
    "    Q2 = cols[1]\n",
    "    \n",
    "    df['f_part'+p]   = df.apply(lambda row: fuzz.partial_ratio (row[Q1], row[Q2]), axis=1)\n",
    "    df['f_Qratio'+p] = df.apply(lambda row: fuzz.QRatio        (row[Q1], row[Q2]), axis=1)\n",
    "    df['f_WRatio'+p] = df.apply(lambda row: fuzz.WRatio        (row[Q1], row[Q2]), axis=1)\n",
    "    df['f_part_set'+p]   = df.apply(lambda row: fuzz.partial_token_set_ratio (row[Q1], row[Q2]), axis=1)\n",
    "    df['f_part_sort'+p]  = df.apply(lambda row: fuzz.partial_token_sort_ratio(row[Q1], row[Q2]), axis=1)\n",
    "    df['f_token_set'+p]  = df.apply(lambda row: fuzz.token_set_ratio         (row[Q1], row[Q2]), axis=1)\n",
    "    df['f_token_sort'+p] = df.apply(lambda row: fuzz.token_sort_ratio        (row[Q1], row[Q2]), axis=1)\n",
    "    \n",
    "    # Select only added columns.\n",
    "    columns_to_return = list(set(df.columns) - set(cols))\n",
    "    \n",
    "    # In order to return several columns to initial DataFrame you need to concatenate this output DataFrame \n",
    "    # with the main one. For example \n",
    "    # data = pd.concat((data, fuzzy_features(data[['q1_src', 'q2_src']])), axis=1) \n",
    "    # returns data with added columns_to_return features.\n",
    "    return df[columns_to_return]\n",
    "\n",
    "def add_simhash_dist(df):\n",
    "    \"\"\"\n",
    "    Input: pandas DataFrame. Must have two columns, corresponding to two questions\n",
    "    Output: pandas Series, consisting of Hamming distances on words/chars X-grams values \n",
    "    (check global variables: NGRAM_TOKEN and NGRAM_VALUE)\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    Q1 = df[cols[0]]\n",
    "    Q2 = df[cols[1]]\n",
    "    def simhash_dist(q1, q2):\n",
    "        if NGRAM_TOKEN=='word':\n",
    "            q1 = q1.split(' ')\n",
    "            q2 = q2.split(' ')\n",
    "        q1 = [' '.join(ngram) for ngram in nltk.ngrams(q1, NGRAM_VALUE)]\n",
    "        q2 = [' '.join(ngram) for ngram in nltk.ngrams(q2, NGRAM_VALUE)]\n",
    "        return Simhash(q1).distance(Simhash(q2))\n",
    "    \n",
    "    df['result'] = [simhash_dist(q1,q2) for q1,q2 in zip(Q1, Q2)]\n",
    "    return df['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuzzywuzzy dists for _src postfix are done in 0.1 minutes\n",
      "simhash dists for _src postfix are done in 0.0 minutes\n",
      "minhash dists for _src postfix are done in 0.1 minutes\n",
      "other dists for _src postfix are done in 0.0 minutes\n",
      "fuzzywuzzy dists for _src postfix are done in 0.1 minutes\n",
      "simhash dists for _src postfix are done in 0.0 minutes\n",
      "minhash dists for _src postfix are done in 0.1 minutes\n",
      "other dists for _src postfix are done in 0.0 minutes\n",
      "_src postfix is done in 0.3 minutes \n",
      "\n",
      "fuzzywuzzy dists for _stem postfix are done in 0.1 minutes\n",
      "simhash dists for _stem postfix are done in 0.0 minutes\n",
      "minhash dists for _stem postfix are done in 0.1 minutes\n",
      "other dists for _stem postfix are done in 0.0 minutes\n",
      "fuzzywuzzy dists for _stem postfix are done in 0.1 minutes\n",
      "simhash dists for _stem postfix are done in 0.0 minutes\n",
      "minhash dists for _stem postfix are done in 0.1 minutes\n",
      "other dists for _stem postfix are done in 0.0 minutes\n",
      "_stem postfix is done in 0.3 minutes \n",
      "\n",
      "fuzzywuzzy dists for _nostops postfix are done in 0.0 minutes\n",
      "simhash dists for _nostops postfix are done in 0.0 minutes\n",
      "minhash dists for _nostops postfix are done in 0.1 minutes\n",
      "other dists for _nostops postfix are done in 0.0 minutes\n",
      "fuzzywuzzy dists for _nostops postfix are done in 0.0 minutes\n",
      "simhash dists for _nostops postfix are done in 0.0 minutes\n",
      "minhash dists for _nostops postfix are done in 0.1 minutes\n",
      "other dists for _nostops postfix are done in 0.0 minutes\n",
      "_nostops postfix is done in 0.2 minutes \n",
      "\n",
      "CPU times: user 2.16 s, sys: 2.29 s, total: 4.45 s\n",
      "Wall time: 42.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def add_distances(data, p):\n",
    "    \"apply defined functions to data (pd DataFrame) and given postfix\"\n",
    "    \n",
    "    # fuzzywuzzy distances\n",
    "    block_start_time = time.time()\n",
    "    data = pd.concat((data, \n",
    "                      apply_parallel(data[['q1'+p, 'q2'+p]], fuzzy_features)),\n",
    "                      axis=1)\n",
    "    print ('fuzzywuzzy dists for {} postfix are done in {} minutes'\n",
    "           .format(p, round((time.time()-block_start_time)/60,1) ))\n",
    "    \n",
    "    # simhash distances\n",
    "    block_start_time = time.time()\n",
    "    global NGRAM_TOKEN, NGRAM_VALUE\n",
    "    NGRAM_TOKEN = 'word'\n",
    "    NGRAM_VALUE = 3\n",
    "    data['simhash_word3'+p] = apply_parallel(data[['q1'+p, 'q2'+p]], add_simhash_dist)\n",
    "\n",
    "    NGRAM_TOKEN = 'char'\n",
    "    NGRAM_VALUE = 3\n",
    "    data['simhash_char3'+p] = apply_parallel(data[['q1'+p, 'q2'+p]], add_simhash_dist)\n",
    "    print ('simhash dists for {} postfix are done in {} minutes'\n",
    "           .format(p, round((time.time()-block_start_time)/60,1) ))    \n",
    "    \n",
    "    # minhash distances\n",
    "    # comment next four lines to save 99% of time execution\n",
    "    block_start_time = time.time()\n",
    "    data['mhash2'+p] = apply_parallel(data[['q1'+p, 'q2'+p]], minhash_sim_2gram)\n",
    "    data['mhash3'+p] = apply_parallel(data[['q1'+p, 'q2'+p]], minhash_sim_3gram)\n",
    "    print ('minhash dists for {} postfix are done in {} minutes'\n",
    "           .format(p, round((time.time()-block_start_time)/60,1) ))\n",
    "    \n",
    "    # additional basic distances\n",
    "    block_start_time = time.time()\n",
    "    data['diffl'+p]  = [difflib.SequenceMatcher(None, q1, q2).ratio() for q1,q2 in zip(data['q1'+p], data['q2'+p])]\n",
    "    data['sor'+p]    = [1 - distance.sorensen(q1, q2)                 for q1,q2 in zip(data['q1'+p], data['q2'+p])]\n",
    "    data['jac'+p]    = [1 - distance.jaccard(q1, q2)                  for q1,q2 in zip(data['q1'+p], data['q2'+p])]\n",
    "    print ('other dists for {} postfix are done in {} minutes'\n",
    "           .format(p, round((time.time()-block_start_time)/60,1) ))\n",
    "\n",
    "    return data\n",
    "\n",
    "for p in postfixes:\n",
    "    t_start = time.time()\n",
    "    data = add_distances(data, p)\n",
    "    kagg = add_distances(kagg, p)\n",
    "    print ('{} postfix is done in {} minutes \\n'.format(p, round((time.time()-t_start)/60,1) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word mover distance on GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 39s, sys: 40.9 s, total: 7min 20s\n",
      "Wall time: 7min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim\n",
    "\n",
    "# read GoogleNews word2vec embeddings with 300 vectors for each word.\n",
    "# first model will be used as is, but the second is normalized\n",
    "w2v_model      = gensim.models.KeyedVectors.load_word2vec_format('GloVe/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "norm_w2v_model = gensim.models.KeyedVectors.load_word2vec_format('GloVe/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "norm_w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def wmd(df):\n",
    "    \"\"\"\n",
    "    Input: pandas DataFrame. Must have two columns, corresponding to two questions\n",
    "    Output: pandas Series, containing word mover distance on GloVe word embeddings\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    Q1 = df[cols[0]]\n",
    "    Q2 = df[cols[1]]\n",
    "    df['res'] = [w2v_model.wmdistance(q1.split(' '), q2.split(' ')) for q1,q2 in zip(Q1,Q2)]\n",
    "    return df['res']\n",
    "\n",
    "def wmd_norm(df):\n",
    "    \"\"\"\n",
    "    Input: pandas DataFrame. Must have two columns, corresponding to two questions\n",
    "    Output: pandas Series, containing word mover distance on normalized GloVe word embeddings\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    Q1 = df[cols[0]]\n",
    "    Q2 = df[cols[1]]\n",
    "    df['res'] = [norm_w2v_model.wmdistance(q1.split(' '), q2.split(' ')) for q1,q2 in zip(Q1,Q2)]\n",
    "    return df['res']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_src\n",
      "_stem\n",
      "_nostops\n",
      "CPU times: user 640 ms, sys: 1.14 s, total: 1.78 s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Adding wordmover distances on GloVe embeddings\n",
    "for p in postfixes:\n",
    "       \n",
    "    data['wmd_'+p]  = apply_parallel(data[['q1'+p, 'q2'+p]], wmd)\n",
    "    data['wmdn_'+p] = apply_parallel(data[['q1'+p, 'q2'+p]], wmd_norm)\n",
    "    \n",
    "    kagg['wmd_'+p]  = apply_parallel(kagg[['q1'+p, 'q2'+p]], wmd)\n",
    "    kagg['wmdn_'+p] = apply_parallel(kagg[['q1'+p, 'q2'+p]], wmd_norm)\n",
    "    \n",
    "    print (p)\n",
    "    \n",
    "del w2v_model, norm_w2v_model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_src postfix is done in 0.0 minutes \n",
      "\n",
      "_stem postfix is done in 0.0 minutes \n",
      "\n",
      "_nostops postfix is done in 0.0 minutes \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def intersect_full_col(df):\n",
    "    \"\"\"\n",
    "    Input: pandas DataFrame. Must have two columns, corresponding to two questions\n",
    "    Output: pandas Series, consisting of jaccard distance on full word intersection between two questions\n",
    "    (see description for intersect_words function below)\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    Q1 = df[cols[0]]\n",
    "    Q2 = df[cols[1]]\n",
    "    \n",
    "    def word_sublist(w):\n",
    "        \"\"\"\n",
    "        input: list of words\n",
    "        returns: generator for all possible ordered word sequences from list. \n",
    "        E.g. ['a', 'b', 'c'] -> [('a',), ('a', 'b'), ('a', 'b', 'c'), ('b',), ('b', 'c'), ('c',)]\n",
    "        \"\"\"\n",
    "        for i in range(len(w)):\n",
    "            for j in range(i, len(w)):\n",
    "                yield tuple(w[i:j+1])\n",
    "    \n",
    "    def intersect_words(q1, q2):\n",
    "        \"\"\"\n",
    "        input: two strings with questions\n",
    "        output: intersection character length divided by total number of character in both questions\n",
    "        E.g. \n",
    "            input: q1 = 'a b c d e g'; q2 = 'h a b c a d e'\n",
    "            max intersetion = 'a b c'\n",
    "            output: 3/13\n",
    "        \"\"\"\n",
    "        words1 = q1.split(' ')\n",
    "        words2 = q2.split(' ')\n",
    "\n",
    "        intersections = set(word_sublist(words1)) & set(word_sublist(words2))\n",
    "        if len(intersections)>0:\n",
    "            max_intersection = max(intersections, key=lambda x: len(x))\n",
    "            char_sum = sum([len(i) for i in max_intersection])\n",
    "            return char_sum/len(q1+q2)\n",
    "        return 0\n",
    "    \n",
    "    df['result'] = [intersect_words(q1,q2) for q1,q2 in zip(Q1, Q2)]\n",
    "    return df['result'].replace('', 'xxx')\n",
    "\n",
    "def stopshare(q, q_nostops):\n",
    "    \"\"\"\n",
    "    input: two strings: full question and question with stop-words removed (look up remove_stopwords_col funciton)\n",
    "    output: share of stopwords in full question. -1 if question is empty\n",
    "    \"\"\"\n",
    "    if len(q)>0:\n",
    "        return (len(q)-len(q_nostops))/len(q)\n",
    "    return -1 \n",
    "\n",
    "def add_len_features(data, p):\n",
    "    \"\"\"\n",
    "    input: DataFrame with questions columns name in format q1+postfix and q2+postfix.\n",
    "    output: DataFrame with added basic word and char lengths features for questions, their intersection and resudial\n",
    "    \"\"\"\n",
    "    \n",
    "    data['q1_word_len'+p]    = data['q1'+p].apply(lambda q: len(q.split(' ')))\n",
    "    data['q2_word_len'+p]    = data['q2'+p].apply(lambda q: len(q.split(' ')))\n",
    "    data['inter_word_len'+p] = data['inter'+p].apply(lambda q: len(q.split(' ')))\n",
    "    data['extra_word_len'+p] = data['extra'+p].apply(lambda q: len(q.split(' ')))\n",
    "    \n",
    "    data['q1_char_len'+p]    = data['q1'+p].apply(len)\n",
    "    data['q2_char_len'+p]    = data['q2'+p].apply(len)\n",
    "    data['inter_char_len'+p] = data['inter'+p].apply(len)\n",
    "    data['extra_char_len'+p] = data['extra'+p].apply(len)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def pos_diff_from_start_col(df):\n",
    "    \"\"\"\n",
    "    Input: pandas DataFrame. Must have two columns, corresponding to two questions\n",
    "    Output: pandas Series, containing custom word positional diffirence metric\n",
    "    (see description for positional_diff function below)\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    Q1 = df[cols[0]]\n",
    "    Q2 = df[cols[1]]\n",
    "    Inter = df[cols[2]]\n",
    "\n",
    "    def positional_diff(q1,q2,inter):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            q1: string, text for the first question\n",
    "            q2: string, text for the second question\n",
    "            inter: shared words for questions\n",
    "        Output: pandas Series, containing custom word positional diffirence metric\n",
    "        Calculation: for each word in interseciton we find it's index in the each question. Then we sum up \n",
    "        absolute index diffirences and divide resulting sum by average len of the question. \n",
    "        The higher the value, the more mixed the word order is.\n",
    "        \"\"\"\n",
    "        if inter=='xxx':\n",
    "            return 1\n",
    "\n",
    "        inter = inter.split(' ')\n",
    "        q1 = q1.split(' ')\n",
    "        q2 = q2.split(' ')\n",
    "        len_q1 = len(q1)\n",
    "        len_q2 = len(q2)\n",
    "\n",
    "        diff_sum = 0\n",
    "        for word in inter:\n",
    "            q1_pos = q1.index(word)\n",
    "            q2_pos = q2.index(word)\n",
    "            diff_sum += abs(q1_pos-q2_pos)\n",
    "\n",
    "        diff_sum /= len(inter)\n",
    "        diff_sum /= (len(q1)+len(q2))/2\n",
    "        return diff_sum\n",
    "    \n",
    "    df['result'] = [positional_diff(q1, q2, inter) \n",
    "                            for q1,q2,inter \n",
    "                            in zip(Q1,Q2,Inter)]\n",
    "    return df['result']\n",
    "\n",
    "def pos_diff_from_end_col(df):\n",
    "    \"\"\"\n",
    "    Input: pandas DataFrame. Must have two columns, corresponding to two questions\n",
    "    Output: pandas Series, containing custom word positional diffirence metric\n",
    "    (see description for positional_diff function below)\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    Q1 = df[cols[0]]\n",
    "    Q2 = df[cols[1]]\n",
    "    Inter = df[cols[2]]\n",
    "\n",
    "    def positional_diff(q1,q2,inter):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            q1: string, text for the first question\n",
    "            q2: string, text for the second question\n",
    "            inter: shared words for questions\n",
    "        Output: pandas Series, containing custom word positional diffirence metric\n",
    "        Calculation: for each word in interseciton we find it's index FROM THE QUESTION END in the each question. \n",
    "        Then we sum up absolute index diffirences and divide resulting sum by average len of the question. \n",
    "        The higher the value, the more mixed the word order is.\n",
    "        \"\"\"\n",
    "        if inter=='xxx':\n",
    "            return 1\n",
    "\n",
    "        inter = inter.split(' ')\n",
    "        q1 = q1.split(' ')\n",
    "        q2 = q2.split(' ')\n",
    "        len_q1 = len(q1)\n",
    "        len_q2 = len(q2)\n",
    "\n",
    "        diff_sum = 0\n",
    "        for word in inter:\n",
    "            q1_pos = q1.index(word) - len_q1\n",
    "            q2_pos = q2.index(word) - len_q2\n",
    "            diff_sum += abs(q1_pos-q2_pos)\n",
    "\n",
    "        diff_sum /= len(inter)\n",
    "        diff_sum /= (len(q1)+len(q2))/2\n",
    "        return diff_sum\n",
    "    \n",
    "    df['result'] = [positional_diff(q1, q2, inter) \n",
    "                            for q1,q2,inter \n",
    "                            in zip(Q1,Q2,Inter)]\n",
    "    return df['result']\n",
    "\n",
    "\n",
    "\n",
    "def add_customs(data, p):\n",
    "    \"apply defined functions to data (pd DataFrame) and given postfix\"\n",
    "    \n",
    "    data['full_inter'+p]   = apply_parallel(data[['q1'+p, 'q2'+p]], intersect_full_col)\n",
    "    data['pos_diff'+p]     = apply_parallel(data[['q1'+p, 'q2'+p, 'inter'+p]], pos_diff_from_start_col)\n",
    "    data['pos_diff_end'+p] = apply_parallel(data[['q1'+p, 'q2'+p, 'inter'+p]], pos_diff_from_end_col)\n",
    "    \n",
    "    if p =='q1_stopshare':\n",
    "        data['q1_stopshare'] = [stopshare(q, q_nostops) for q, q_nostops in zip(data.q1_src, data.q1_nostops)]\n",
    "        data['q2_stopshare'] = [stopshare(q, q_nostops) for q, q_nostops in zip(data.q2_src, data.q2_nostops)]\n",
    "    data = add_len_features(data, p)\n",
    "    return data\n",
    "    \n",
    "for p in postfixes:\n",
    "    t_start = time.time()\n",
    "    data = add_customs(data, p)\n",
    "    kagg = add_customs(kagg, p)    \n",
    "    print ('{} postfix is done in {} minutes \\n'.format(p, round((time.time()-t_start)/60,1) ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_src postfix is done in 0.0 minutes\n",
      "_stem postfix is done in 0.0 minutes\n",
      "_nostops postfix is done in 0.0 minutes\n",
      "CPU times: user 484 ms, sys: 8.18 ms, total: 492 ms\n",
      "Wall time: 495 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def question_uniquness(q, word_count):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        q - string, question\n",
    "        word_count - dictionary with word coutns for each word in train dataset\n",
    "    output: mean uniquness of words in question. ranges from 0.33 (very unique) to 0 (very common)\n",
    "    \"\"\"\n",
    "    \n",
    "    words = q.split(' ')\n",
    "    \n",
    "    # Number of words in question with count >=3. Incremented for each word satisfying the condition\n",
    "    word_number = 0 \n",
    "    \n",
    "    # Uniqueness index of question. Incremented by 1.0/word_count for each word in question. \n",
    "    # Rarest words will give 1/3 and the most common will give value close to zero.\n",
    "    word_sum = 0 \n",
    "\n",
    "    # For each word in questions get it's count from train dataset. If 3 or greater, add 1/count to word_sum \n",
    "    # and 1 to word_number.\n",
    "    for word in words:\n",
    "        val = word_count.get(word,0)\n",
    "        if val>=3:\n",
    "            word_sum += 1.0/val\n",
    "            word_number += 1\n",
    "            \n",
    "    # Return word_sum/word_number. Return 0.35 if word_number is zero.       \n",
    "    if word_number>0:\n",
    "        return word_sum/word_number #\n",
    "    return 0.35\n",
    "\n",
    "def add_uniquness_features(data, p, word_count):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        data - DataFrame with all columns\n",
    "        p - postfix\n",
    "        word_count - dictionary with word coutns for each word in train dataset\n",
    "    output: main DataFrame with question_uniquness function applied to questions, their intersection and resudial\n",
    "    \"\"\"    \n",
    "    \n",
    "    data['q1_uniq'+p]    = data['q1'+p]   .apply(lambda q: question_uniquness(q, word_count))\n",
    "    data['q2_uniq'+p]    = data['q2'+p]   .apply(lambda q: question_uniquness(q, word_count))\n",
    "    data['inter_uniq'+p] = data['inter'+p].apply(lambda q: question_uniquness(q, word_count))\n",
    "    data['extra_uniq'+p] = data['extra'+p].apply(lambda q: question_uniquness(q, word_count))\n",
    "    return data\n",
    "\n",
    "\n",
    "for p in postfixes:\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # Compute word counts dictionary for train dataset. Recomputed for each postfix\n",
    "    word_count = dict()\n",
    "    for _, row in data.iterrows():\n",
    "        q1 = row['q1'+p].split(' ')\n",
    "        q2 = row['q2'+p].split(' ')\n",
    "        for w in (q1+q2):\n",
    "            word_count[w] = word_count.get(w, 0)+1\n",
    "    \n",
    "    # Add uniquness features to DataFrames\n",
    "    data = add_uniquness_features(data, p, word_count)\n",
    "    kagg = add_uniquness_features(kagg, p, word_count)\n",
    "        \n",
    "    print ('{} postfix is done in {} minutes'.format(p, round((time.time()-t_start)/60,1) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom features: positional differences matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.47 s, sys: 23.3 ms, total: 1.49 s\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def pos_diff_matrix(df, max_len, prefix, from_end):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        df: pd DataFrame, that contains two question columns. Must not contain Nulls\n",
    "        max_len: int, max word length of question. Every question will be truncated to that word length\n",
    "        prefix: string, column name prefix.\n",
    "        from_end: boolean, if False - questions are truncated by max_len from start. Else - from end.\n",
    "    output: pd DataFrame with max_len columns. Each column has values from 0 to max_len*10\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = df.columns\n",
    "    Q1 = df[cols[0]]\n",
    "    Q2 = df[cols[1]]\n",
    "    \n",
    "    def text_intersect(q1, q2, max_len):\n",
    "        \"\"\"\n",
    "        input: two questions in strings, number of words to which truncate the questions\n",
    "        output: list of max_len elements, that represent ordinal word differences in quesiton (see comments in code)\n",
    "        \"\"\"\n",
    "\n",
    "        # For each word in the first question get it's position number in quesiton and \n",
    "        # create dict where word is a key and it's position is a value.\n",
    "        q1_words_positions = dict()\n",
    "        word_position = 0\n",
    "        q1_word_list = q1.split(' ')\n",
    "        if from_end:\n",
    "            q1_word_list = reversed(q1_word_list)\n",
    "        for word in q1_word_list:\n",
    "            if word not in q1_words_positions:\n",
    "                q1_words_positions[word] = word_position\n",
    "                word_position += 1\n",
    "        \n",
    "        # Get truncated to max_len list of words in second question.\n",
    "        q2_word_list = q2.split(' ')\n",
    "        if from_end:\n",
    "            q2_word_list = list(reversed(q2_word_list))\n",
    "        q2_word_list = q2_word_list[:max_len]\n",
    "        \n",
    "        # For each word in second question calculate it's position and compute it's positional difference \n",
    "        # with the same word in the first question. If the word is not present in the first question, it's position\n",
    "        # in the first question is set to 5*max_len. \n",
    "        intersection = []\n",
    "        for i in range(len(q2_word_list)):\n",
    "            intersection.append(q1_words_positions.get(q2_word_list[i], max_len*5) - i)\n",
    "        \n",
    "        # Fill resulted numbers with max_len*10 values to the length of max_len. Happens if second question \n",
    "        # is shorter that max_len.\n",
    "        intersection += [max_len*10]*(max_len-len(intersection))\n",
    "        return intersection\n",
    "    \n",
    "    # Apply text_intersect to every questions pair in df. Return pd DataFrame\n",
    "    matrix = []\n",
    "    for q1,q2 in zip(Q1,Q2):\n",
    "        matrix.append(text_intersect(q1, q2, max_len))\n",
    "    return pd.DataFrame(matrix, columns = [prefix+str(i+1) for i in range(max_len)])\n",
    "\n",
    "assert data.dropna().shape == data.shape\n",
    "MAX_WORDS_IN_QUESTION = 10\n",
    "pos_diff_postfixes = ['_nostops', '_stem']\n",
    "\n",
    "# Calculate posdiff matricies for each postfix in pos_diff_postfixes from start and from eng. \n",
    "# Concatenate every matrix into single pd DataFrame. Resulting number of columns is:\n",
    "# MAX_WORDS_IN_QUESTION*len(pos_diff_postfixes)*2 = 40 for current parameters.\n",
    "data_posdiff = pd.DataFrame()\n",
    "kagg_posdiff = pd.DataFrame()\n",
    "for p in pos_diff_postfixes:\n",
    "    data_posdiff = pd.concat((data_posdiff, pos_diff_matrix(data[['q1'+p, 'q2'+p]], MAX_WORDS_IN_QUESTION, \n",
    "                                       'posdiff'+p, from_end=False),\n",
    "                               pos_diff_matrix(data[['q1'+p, 'q2'+p]], MAX_WORDS_IN_QUESTION, \n",
    "                                       'posdiff_end'+p, from_end=True)), \n",
    "                               axis=1, copy=False)\n",
    "    \n",
    "    kagg_posdiff = pd.concat((pos_diff_matrix(kagg[['q1'+p, 'q2'+p]], MAX_WORDS_IN_QUESTION, \n",
    "                                       'posdiff'+p, from_end=False),\n",
    "                               pos_diff_matrix(kagg[['q1'+p, 'q2'+p]], MAX_WORDS_IN_QUESTION, \n",
    "                                       'posdiff_end'+p, from_end=True)), \n",
    "                               axis=1, copy=False)\n",
    "\n",
    "assert data_posdiff.dropna().shape == data_posdiff.shape\n",
    "\n",
    "#data_posdiff.to_csv('data/features/train_pos_diff_matrix.csv', index=False)\n",
    "#kagg_posdiff.to_csv('data/features/test_pos_diff_matrix.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "40000 40000 37295 37255\n"
     ]
    }
   ],
   "source": [
    "# reload unprocessed questions. We'll need to get all words starting with capital letter, and sentences' endings\n",
    "data_qs = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'), usecols=[3,4])\n",
    "data_qs.columns = ['q1', 'q2']\n",
    "data_qs.fillna('xxx', inplace=True)\n",
    "\n",
    "kagg_qs = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'),  usecols=[1,2])\n",
    "kagg_qs.columns = ['q1', 'q2']\n",
    "kagg_qs.fillna('xxx', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 118 ms, sys: 99.7 ms, total: 218 ms\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def sentence_regex(text):    \n",
    "    \"\"\"\n",
    "    input: text string\n",
    "    output: string with some common English abbrv replaced. Doesn't remove punctuation.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"\\'s\", \" 's \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \" cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\";\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def get_proper_nouns_list(data):\n",
    "    \"\"\"\n",
    "    input: pd DataFrame with a single column, in which questions are stored in string format\n",
    "    output: pd Series with list of words, that are starting with capital letter in the middle of the sentence\n",
    "    for each quesiton\n",
    "    \"\"\"\n",
    "    \n",
    "    q = data.columns[0]\n",
    "    \n",
    "    # Split question in a list of sentences\n",
    "    data[q]  = data[q].apply(lambda Q: tokenizer.tokenize(Q))\n",
    "    \n",
    "    # Clean up every sentence with regexes\n",
    "    data[q] = data[q].apply(lambda Q: [sentence_regex(S) for S in Q])\n",
    "    \n",
    "    # In each sentences take all words except for the first one. If word length >1 and the first char in upper case\n",
    "    # then add it to the list of proper nouns in question\n",
    "    data[q+'_cap'] = data[q].apply(lambda Q: \n",
    "                                   list(flatten([\n",
    "                                       [word for word in S.split(' ')[1:] if len(word)>1 and word[0].isupper()] \n",
    "                                                  for S in Q])))\n",
    "    return data[q+'_cap']\n",
    "\n",
    "data_qs['q1_cap'] = apply_parallel(data_qs[['q1']], get_proper_nouns_list)\n",
    "data_qs['q2_cap'] = apply_parallel(data_qs[['q2']], get_proper_nouns_list)\n",
    "\n",
    "kagg_qs['q1_cap'] = apply_parallel(kagg_qs[['q1']], get_proper_nouns_list)\n",
    "kagg_qs['q2_cap'] = apply_parallel(kagg_qs[['q2']], get_proper_nouns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 97.3 ms, sys: 74.2 ms, total: 171 ms\n",
      "Wall time: 2.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def add_propernouns_features(data):\n",
    "    \"\"\"\n",
    "    input: pd DataFrame, that contains following columns:\n",
    "        'q1' and 'q2' - questions in string format\n",
    "        'q1_cap' and 'q2_cap' - lists of words, that starts with capital letter (proper nouns)\n",
    "    output: pd DataFrame with 6 features: lengths of proper nouns for both questions, their intersection \n",
    "    and difference; relative length of intersection and difference\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for index, row in data.iterrows():\n",
    "        q1_cap = row['q1_cap']\n",
    "        q2_cap = row['q2_cap']\n",
    "\n",
    "        # For each first word of each sentence in q1, check if the word is in proper nouns of q2.\n",
    "        # If it is - add it to lsit of q1 proper nouns. \n",
    "        # Reason for it - we excluded first words on previous step.\n",
    "        for S in row['q1']:\n",
    "            first_word = S.split(' ')[0]\n",
    "            if first_word in q2_cap:\n",
    "                q1_cap.append(first_word)\n",
    "        # Vice versa\n",
    "        for S in row['q2']:\n",
    "            first_word = S.split(' ')[0]\n",
    "            if first_word in q1_cap:\n",
    "                q2_cap.append(first_word)\n",
    "        \n",
    "        # Get list of matches between proper nouns list\n",
    "        proper_matches = list(set(q1_cap).intersection(set(q2_cap)))\n",
    "    \n",
    "        # Dict, from which the row will be constructed at the return\n",
    "        row_dict = {}\n",
    "        \n",
    "        row_dict['PN_q1_count'] = len(q1_cap)\n",
    "        row_dict['PN_q2_count'] = len(q2_cap)\n",
    "        row_dict['PN_match_count'] = len(proper_matches)\n",
    "        row_dict['PN_mismatch_count'] = len(set(q1_cap+q2_cap).difference(set(proper_matches)))\n",
    "        \n",
    "        if len(q1_cap) + len(q2_cap) > 0:\n",
    "            row_dict['PN_match_relative']   = len(proper_matches) / (len(q1_cap)+len(q2_cap))\n",
    "            row_dict['PN_mismatch_relative'] = row_dict['PN_mismatch_count'] / ( len(q1_cap)+len(q2_cap))\n",
    "        else:\n",
    "            row_dict['PN_match_relative'] = -1\n",
    "            row_dict['PN_mismatch_relative'] = -1\n",
    "\n",
    "        results.append(row_dict)\n",
    "    return pd.DataFrame.from_dict(results) # returns pd DataFrame\n",
    "\n",
    "# Adding proper nouns features\n",
    "data_PN = apply_parallel(data_qs[['q1', 'q2', 'q1_cap', 'q2_cap']], add_propernouns_features)\n",
    "kagg_PN = apply_parallel(kagg_qs[['q1', 'q2', 'q1_cap', 'q2_cap']], add_propernouns_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique countries: 175\n",
      "unique cities: 3489\n"
     ]
    }
   ],
   "source": [
    "# Get string of countries and cities. We will search for shared locations\n",
    "\n",
    "locations = pd.read_csv(\"locations/cities.csv\")\n",
    "\n",
    "countries = \"|\".join(set(locations['Country'].dropna()))\n",
    "cities    = \"|\".join(set(locations['City'].dropna()))\n",
    "cities = re.sub(r\"[^A-Za-z0-9,-|]\", \" \", cities) # Clean up the mess\n",
    "\n",
    "print ('unique countries:', locations['Country'].nunique())\n",
    "print ('unique cities:', locations['City'].nunique())\n",
    "\n",
    "del locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70.4 ms, sys: 59.7 ms, total: 130 ms\n",
      "Wall time: 5.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def add_country_features(data):\n",
    "    \"\"\"\n",
    "    input: pd DataFrame, that contains following columns:\n",
    "        'q1' and 'q2' - questions in string format\n",
    "    output: pd DataFrame with 6 features: number of countries for both questions, their intersection \n",
    "    and difference; relative number of intersection and difference numbers\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for _, row in data.iterrows():\n",
    "        q1 = row['q1']\n",
    "        q2 = row['q2']\n",
    "        \n",
    "        # Get lists of mentioned countries for each question\n",
    "        q1_countries = [country.lower() for country in re.findall(countries, q1, flags=re.IGNORECASE)]\n",
    "        q2_countries = [country.lower() for country in re.findall(countries, q2, flags=re.IGNORECASE)]\n",
    "        # Intersection of countries\n",
    "        country_mathces = set(q1_countries).intersection(set(q2_countries))\n",
    "        \n",
    "        # Dict, from which the row will be constructed at the return\n",
    "        row_dict = {}\n",
    "        row_dict['loc_q1_country_num'] = len(q1_countries)\n",
    "        row_dict['loc_q2_country_num'] = len(q2_countries) \n",
    "        row_dict['loc_country_match_num'] = len(country_mathces)\n",
    "        row_dict['loc_country_mismatch_num'] = len(set(q1_countries).difference(set(q2_countries)))\n",
    "        \n",
    "        if len(q1_countries) + len(q2_countries) > 0:\n",
    "            row_dict['loc_country_match_relative'] = len(country_mathces) / (len(q1_countries)+len(q2_countries))\n",
    "            row_dict['loc_country_mismatch_relative'] = (row_dict['loc_country_mismatch_num'] \n",
    "                                                         / (len(q1_countries)+len(q2_countries)))\n",
    "        else:\n",
    "            row_dict['loc_country_match_relative'] = -1\n",
    "            row_dict['loc_country_mismatch_relative'] = -1\n",
    "\n",
    "        results.append(row_dict)     \n",
    "    return pd.DataFrame.from_dict(results) # returns pd DataFrame\n",
    "\n",
    "data_PN = pd.concat([data_PN\n",
    "                    ,apply_parallel(data_qs[['q1', 'q2']], add_country_features)]\n",
    "                    ,axis=1)\n",
    "kagg_PN = pd.concat([kagg_PN\n",
    "                    ,apply_parallel(kagg_qs[['q1', 'q2']], add_country_features)]\n",
    "                    ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 292 ms, sys: 195 ms, total: 487 ms\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def add_city_features(data):\n",
    "    \"\"\"\n",
    "    input: pd DataFrame, that contains following columns:\n",
    "        'q1' and 'q2' - questions in string format\n",
    "    output: pd DataFrame with 6 features: number of cities for both questions, their intersection \n",
    "    and difference; relative number of intersection and difference numbers\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for _, row in data.iterrows():\n",
    "        q1 = row['q1']\n",
    "        q2 = row['q2']\n",
    "        \n",
    "        q1_cities = [city.lower() for city in re.findall(cities, q1, flags=re.IGNORECASE)]\n",
    "        q2_cities = [city.lower() for city in re.findall(cities, q2, flags=re.IGNORECASE)]\n",
    "        city_mathces = set(q1_cities).intersection(set(q2_cities))\n",
    "        \n",
    "        row_dict = {}\n",
    "        row_dict['loc_q1_city_num'] = len(q1_cities)\n",
    "        row_dict['loc_q2_city_num'] = len(q2_cities) \n",
    "        row_dict['loc_city_match_num'] = len(city_mathces)\n",
    "        row_dict['loc_city_mismatch_num'] = len(set(q1_cities).difference(set(q2_cities)))\n",
    "        \n",
    "        if len(q1_cities) + len(q2_cities) > 0:\n",
    "            row_dict['loc_city_match_relative'] = len(city_mathces) / (len(q1_cities)+len(q2_cities))\n",
    "            row_dict['loc_city_mismatch_relative'] = (row_dict['loc_city_mismatch_num'] \n",
    "                                                         / (len(q1_cities)+len(q2_cities)))\n",
    "        else:\n",
    "            row_dict['loc_city_match_relative'] = -1\n",
    "            row_dict['loc_city_mismatch_relative'] = -1\n",
    "\n",
    "        results.append(row_dict)     \n",
    "    return pd.DataFrame.from_dict(results)\n",
    "\n",
    "data_PN = pd.concat([data_PN\n",
    "                    ,apply_parallel(data_qs[['q1', 'q2']], add_city_features)]\n",
    "                    ,axis=1)\n",
    "kagg_PN = pd.concat([kagg_PN\n",
    "                    ,apply_parallel(kagg_qs[['q1', 'q2']], add_city_features)]\n",
    "                    ,axis=1)\n",
    "\n",
    "data_PN.to_csv('data/features/train_PN.csv', index=False)\n",
    "kagg_PN.to_csv('data/features/test_PN.csv',  index=False)\n",
    "del data_PN, kagg_PN, data_qs, kagg_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard char ngrams (Mephistphel's features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_src\n",
      "_src\n",
      "_stem\n",
      "_stem\n",
      "_nostops\n",
      "_nostops\n"
     ]
    }
   ],
   "source": [
    "# Jaccard distances for char ngrams\n",
    "# Source: https://www.kaggle.com/c/quora-question-pairs/discussion/32313\n",
    "\n",
    "for p in postfixes:\n",
    "    print (p)\n",
    "    #kagg = pd.read_csv(os.path.join(DATA_PATH, 'kagg_feat'+p+'.csv'), sep=';')\n",
    "    kagg.fillna('xxx', inplace=True)\n",
    "    \n",
    "    ### jaccard distances ###\n",
    "    \n",
    "    cv_char = CountVectorizer(ngram_range=(1, 3), analyzer='char')\n",
    "    ch_freq = np.array(cv_char.fit_transform(data['q1'+p].tolist() + data['q2'+p].tolist()).sum(axis=0))[0, :]\n",
    "    \n",
    "    unigrams = dict([(k, v) for (k, v) in cv_char.vocabulary_.items() if len(k) == 1])\n",
    "    ix_unigrams = np.sort(list(unigrams.values()))\n",
    "    bigrams = dict([(k, v) for (k, v) in cv_char.vocabulary_.items() if len(k) == 2])\n",
    "    ix_bigrams = np.sort(list(bigrams.values()))\n",
    "    trigrams = dict([(k, v) for (k, v) in cv_char.vocabulary_.items() if len(k) == 3])\n",
    "    ix_trigrams = np.sort(list(trigrams.values()))\n",
    "    \n",
    "    ####### unigrams ######\n",
    "    \n",
    "    ## train\n",
    "    m_q1 = cv_char.transform(data['q1'+p].values)\n",
    "    m_q2 = cv_char.transform(data['q2'+p].values)\n",
    "    \n",
    "    v_num = (m_q1[:, ix_unigrams] > 0).minimum((m_q2[:, ix_unigrams] > 0)).sum(axis=1)\n",
    "    v_den = (m_q1[:, ix_unigrams] > 0).maximum((m_q2[:, ix_unigrams] > 0)).sum(axis=1)\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    data['unigram_jaccard'+p] = v_score\n",
    "    \n",
    "    v_num = m_q1[:, ix_unigrams].minimum(m_q2[:, ix_unigrams]).sum(axis=1)\n",
    "    v_den = m_q1[:, ix_unigrams].sum(axis=1) + m_q2[:, ix_unigrams].sum(axis=1)\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    data['unigram_all_jaccard'+p] = v_score\n",
    "    \n",
    "    v_num = m_q1[:, ix_unigrams].minimum(m_q2[:, ix_unigrams]).sum(axis=1)\n",
    "    v_den = m_q1[:, ix_unigrams].maximum(m_q2[:, ix_unigrams]).sum(axis=1)\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    data['unigram_all_jaccard_max'+p] = v_score\n",
    "    \n",
    "    ## test\n",
    "    m_q1 = cv_char.transform(kagg['q1'+p].values)\n",
    "    m_q2 = cv_char.transform(kagg['q2'+p].values)\n",
    "    \n",
    "    v_num = (m_q1[:, ix_unigrams] > 0).minimum((m_q2[:, ix_unigrams] > 0)).sum(axis=1)\n",
    "    v_den = (m_q1[:, ix_unigrams] > 0).maximum((m_q2[:, ix_unigrams] > 0)).sum(axis=1)\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    kagg['unigram_jaccard'+p] = v_score\n",
    "    \n",
    "    v_num = m_q1[:, ix_unigrams].minimum(m_q2[:, ix_unigrams]).sum(axis=1)\n",
    "    v_den = m_q1[:, ix_unigrams].sum(axis=1) + m_q2[:, ix_unigrams].sum(axis=1)\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    kagg['unigram_all_jaccard'+p] = v_score\n",
    "    \n",
    "    v_num = m_q1[:, ix_unigrams].minimum(m_q2[:, ix_unigrams]).sum(axis=1)\n",
    "    v_den = m_q1[:, ix_unigrams].maximum(m_q2[:, ix_unigrams]).sum(axis=1)\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    kagg['unigram_all_jaccard_max'+p] = v_score\n",
    "    \n",
    "    \n",
    "    ####### bigrams ######\n",
    "    \n",
    "    ## train\n",
    "    m_q1 = cv_char.transform(data['q1'+p].values)\n",
    "    m_q2 = cv_char.transform(data['q2'+p].values)\n",
    "    \n",
    "    v_num = (m_q1[:, ix_bigrams] > 0).minimum((m_q2[:, ix_bigrams] > 0)).sum(axis=1)\n",
    "    v_den = (m_q1[:, ix_bigrams] > 0).maximum((m_q2[:, ix_bigrams] > 0)).sum(axis=1)\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    data['bigram_jaccard'+p] = v_score\n",
    "    \n",
    "    v_num = m_q1[:, ix_bigrams].minimum(m_q2[:, ix_bigrams]).sum(axis=1)\n",
    "    v_den = m_q1[:, ix_bigrams].sum(axis=1) + m_q2[:, ix_bigrams].sum(axis=1)\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    data['bigram_all_jaccard'+p] = v_score\n",
    "    \n",
    "    v_num = m_q1[:, ix_bigrams].minimum(m_q2[:, ix_bigrams]).sum(axis=1)\n",
    "    v_den = m_q1[:, ix_bigrams].maximum(m_q2[:, ix_bigrams]).sum(axis=1)\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    data['bigram_all_jaccard_max'+p] = v_score\n",
    "    \n",
    "    ##test\n",
    "    m_q1 = cv_char.transform(kagg['q1'+p].values)\n",
    "    m_q2 = cv_char.transform(kagg['q2'+p].values)\n",
    "    \n",
    "    v_num = (m_q1[:, ix_bigrams] > 0).minimum((m_q2[:, ix_bigrams] > 0)).sum(axis=1)\n",
    "    v_den = (m_q1[:, ix_bigrams] > 0).maximum((m_q2[:, ix_bigrams] > 0)).sum(axis=1)\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    kagg['bigram_jaccard'+p] = v_score\n",
    "    \n",
    "    v_num = m_q1[:, ix_bigrams].minimum(m_q2[:, ix_bigrams]).sum(axis=1)\n",
    "    v_den = m_q1[:, ix_bigrams].sum(axis=1) + m_q2[:, ix_bigrams].sum(axis=1)\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    kagg['bigram_all_jaccard'+p] = v_score\n",
    "    \n",
    "    v_num = m_q1[:, ix_bigrams].minimum(m_q2[:, ix_bigrams]).sum(axis=1)\n",
    "    v_den = m_q1[:, ix_bigrams].maximum(m_q2[:, ix_bigrams]).sum(axis=1)\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    kagg['bigram_all_jaccard_max'+p] = v_score\n",
    "    \n",
    "    \n",
    "    ####### trigrams ######\n",
    "    \n",
    "    ## train\n",
    "    m_q1 = cv_char.transform(data['q1'+p].values)\n",
    "    m_q2 = cv_char.transform(data['q2'+p].values)\n",
    "    \n",
    "    v_num = (m_q1[:, ix_trigrams] > 0).minimum((m_q2[:, ix_trigrams] > 0)).sum(axis=1)\n",
    "    v_den = (m_q1[:, ix_trigrams] > 0).maximum((m_q2[:, ix_trigrams] > 0)).sum(axis=1)\n",
    "    v_den[np.where(v_den == 0)] = 1\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    data['trigram_jaccard'+p] = v_score\n",
    "    \n",
    "    v_num = m_q1[:, ix_trigrams].minimum(m_q2[:, ix_trigrams]).sum(axis=1)\n",
    "    v_den = m_q1[:, ix_trigrams].sum(axis=1) + m_q2[:, ix_trigrams].sum(axis=1)\n",
    "    v_den[np.where(v_den == 0)] = 1\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    data['trigram_all_jaccard'+p] = v_score\n",
    "    \n",
    "    v_num = m_q1[:, ix_trigrams].minimum(m_q2[:, ix_trigrams]).sum(axis=1)\n",
    "    v_den = m_q1[:, ix_trigrams].maximum(m_q2[:, ix_trigrams]).sum(axis=1)\n",
    "    v_den[np.where(v_den == 0)] = 1\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    data['trigram_all_jaccard_max'+p] = v_score\n",
    "    \n",
    "    ##test\n",
    "    m_q1 = cv_char.transform(kagg['q1'+p].values)\n",
    "    m_q2 = cv_char.transform(kagg['q2'+p].values)\n",
    "    \n",
    "    v_num = (m_q1[:, ix_trigrams] > 0).minimum((m_q2[:, ix_trigrams] > 0)).sum(axis=1)\n",
    "    v_den = (m_q1[:, ix_trigrams] > 0).maximum((m_q2[:, ix_trigrams] > 0)).sum(axis=1)\n",
    "    v_den[np.where(v_den == 0)] = 1\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    kagg['trigram_jaccard'+p] = v_score\n",
    "    \n",
    "    v_num = m_q1[:, ix_trigrams].minimum(m_q2[:, ix_trigrams]).sum(axis=1)\n",
    "    v_den = m_q1[:, ix_trigrams].sum(axis=1) + m_q2[:, ix_trigrams].sum(axis=1)\n",
    "    v_den[np.where(v_den == 0)] = 1\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    kagg['trigram_all_jaccard'+p] = v_score\n",
    "    \n",
    "    v_num = m_q1[:, ix_trigrams].minimum(m_q2[:, ix_trigrams]).sum(axis=1)\n",
    "    v_den = m_q1[:, ix_trigrams].maximum(m_q2[:, ix_trigrams]).sum(axis=1)\n",
    "    v_den[np.where(v_den == 0)] = 1\n",
    "    v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "    kagg['trigram_all_jaccard_max'+p] = v_score\n",
    "    \n",
    "    \n",
    "    jaccard_feat = ['unigram_jaccard'+p, 'unigram_all_jaccard'+p, 'unigram_all_jaccard_max'+p, \n",
    "                     'bigram_jaccard'+p,  'bigram_all_jaccard'+p,  'bigram_all_jaccard_max'+p,\n",
    "                    'trigram_jaccard'+p, 'trigram_all_jaccard'+p, 'trigram_all_jaccard_max'+p]\n",
    "    \n",
    "    kagg[jaccard_feat].fillna(-1, inplace=True)\n",
    "    data[jaccard_feat].fillna(-1, inplace=True)\n",
    "    \n",
    "    print (p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Synonyms analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'core'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTabError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Users/Denis/anaconda/envs/python3/lib/python3.5/site-packages/PyDictionary/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTabError\u001b[0m: inconsistent use of tabs and spaces in indentation (core.py, line 114)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9fc369cad641>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgoslate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#from PyDictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mPyDictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_parallel_fill_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Denis/anaconda/envs/python3/lib/python3.5/site-packages/PyDictionary/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'core'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import goslate\n",
    "from PyDictionary import PyDictionary\n",
    "\n",
    "def apply_parallel_fill_dict(my_list, my_func):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        my_list: list of words\n",
    "        my_func: custom function which will be apllied to my_list. Must accept list as input and return a dict.\n",
    "    Output: combined dictionary from results of my_func.\n",
    "    \n",
    "    my_list is splitted by the number of cores and function applied to each part independetly.\n",
    "    \"\"\"\n",
    "    list_splitted = np.array_split(my_list, NUM_CORES)\n",
    "    pool = mp.Pool(NUM_CORES)\n",
    "    \n",
    "    result = dict()\n",
    "    for dictionary in pool.map(my_func, list_splitted):\n",
    "        for key, value in dictionary.items():\n",
    "            result[key]=value\n",
    "            \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return result\n",
    "\n",
    "def get_antonyms_dict(words):\n",
    "    \"\"\"\n",
    "    input: list of words\n",
    "    output: dictinoary (word, list_of_antonyms)\n",
    "    \"\"\"\n",
    "    pydict=PyDictionary()\n",
    "    antonym_dict_local = dict()\n",
    "    for word in words:\n",
    "        antonym_list = pydict.antonym(word) # get list of antonyms for the word\n",
    "        if antonym_list: # ignore if None\n",
    "            antonym_dict_local[word] = antonym_list\n",
    "    return antonym_dict_local\n",
    "\n",
    "def get_synonyms_dict(words):\n",
    "    \"\"\"\n",
    "    input: list of words\n",
    "    output: dictinoary (word, list_of_synonyms)\n",
    "    \"\"\"\n",
    "    pydict=PyDictionary()\n",
    "    synonym_dict_local = dict()\n",
    "    for word in words:\n",
    "        synonym_list = pydict.synonym(word) # get list of synonyms for the word\n",
    "        if synonym_list: # ignore if None\n",
    "            synonym_dict_local[word] = synonym_list\n",
    "    return synonym_dict_local\n",
    "\n",
    "# Get array of lists, where each list contain all the words from question pair\n",
    "all_sentences = np.append(\n",
    "                     data[['q1_src', 'q2_src']].iloc[:1000]\n",
    "                     .apply(lambda row: ' '.join((row['q1_src'], row['q2_src'])).split(' '), axis=1)\n",
    "                     .values,\n",
    "    \n",
    "                     kagg[['q1_src', 'q2_src']].iloc[:1000]\n",
    "                     .apply(lambda row: ' '.join((row['q1_src'], row['q2_src'])).split(' '), axis=1)\n",
    "                     .values)\n",
    "\n",
    "# Fill the words dictinary with empty list for each unique word in text\n",
    "words_dict = dict ([(word, []) for word in set(flatten(all_sentences))])\n",
    "print ('total unique words:', len(words_dict))\n",
    "print ('estimated time to end, min:', round(len(words_dict)*(16.5/6479),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "#antonym_dict = apply_parallel_fill_dict(list(words_dict), get_antonyms_dict)\n",
    "#with open('data/dics/antonyms.pkl', 'wb') as F:\n",
    "#    pickle.dump(antonym_dict, F)    \n",
    "\n",
    "synonym_dict = apply_parallel_fill_dict(list(words_dict), get_synonyms_dict)\n",
    "with open('data/dics/synonyms.pkl', 'wb') as F:\n",
    "    pickle.dump(synonym_dict, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def syn(q1, q2):\n",
    "    \"\"\"\n",
    "    input: two questions in string\n",
    "    output: thre feautres:\n",
    "        share of words in questions that doesn't have a synonym in the other quesiton;\n",
    "        share of words, for which synonyms are not defined;\n",
    "        jaccard distance between questions with all words replaced (if possible) to the synonyms that are\n",
    "        present in the other question.\n",
    "    Further info in comments in code\n",
    "    \"\"\"\n",
    "    q1 = q1.split(' ')\n",
    "    q2 = q2.split(' ')\n",
    "    \n",
    "    q1_dict = dict((word, None) for word in q1)\n",
    "    q2_dict = dict((word, None) for word in q2)\n",
    "    \n",
    "    no_syn_count = 0 # Number of words in both questions, that doesn't have a synonym in other quesiton\n",
    "    syn_number = 0   # Number of words in both questions, that have at least one sysnonym in other quesiton\n",
    "    unknown_words_count = 0 # Number of words in both questions with empty synonym list\n",
    "    \n",
    "    # Generate q1_new. Replace each word with the first word from synonym list, that is present in second quesiton.\n",
    "    # Leave word untouched if no matches in second question.\n",
    "    q1_new = []\n",
    "    for w in q1:        \n",
    "        syn_added = syn_number\n",
    "        syn_list = [w] + syn_dict.get(w, []) # current word plus list of synonyms from dictionary\n",
    "        \n",
    "        # Append to q1_new the first word from syn_list, that present in second question.\n",
    "        # If the word is added, increment  number of found synonyms by 1.\n",
    "        for syn_word in syn_list:\n",
    "            if syn_word in q2_dict:\n",
    "                syn_number+=1\n",
    "                q1_new.append(syn_word)\n",
    "                continue\n",
    "        \n",
    "        # If no synonym_word from syn_list is found in second question, then don't replace word with anything\n",
    "        if syn_added == syn_number:\n",
    "            q1_new.append(w)\n",
    "            no_syn_count+=1\n",
    "            \n",
    "        # If synonym_list contain only main word, then increment number of unknown words\n",
    "        if len(syn_list)==1:\n",
    "            unknown_words_count+=1\n",
    "\n",
    "    # Do the same procedure to generate new second quesiton        \n",
    "    q2_new = []\n",
    "    for w in q2:\n",
    "        syn_list = [w] + syn_dict.get(w, [])\n",
    "        syn_added = syn_number\n",
    "        for syn_word in syn_list:\n",
    "            if syn_word in q1_dict:\n",
    "                syn_number+=1\n",
    "                q2_new.append(syn_word)\n",
    "                continue \n",
    "        if syn_added == syn_number:\n",
    "            q2_new.append(w)   \n",
    "        if len(syn_list)==1:\n",
    "            unknown_words_count+=1\n",
    "    \n",
    "    jaccard_distance = len(set(q1_new).intersection(set(q2_new))) / len(set(q1_new+q2_new))\n",
    "    \n",
    "    q1_new = ' '.join(q1_new)\n",
    "    q2_new = ' '.join(q2_new)\n",
    "\n",
    "    mean_length = (len(q1) + len(q2))/2\n",
    "    return pd.Series([q1_new, no_syn_count/mean_length, unknown_words_count/mean_length, jaccard_distance], \n",
    "            index = ['nosyn_share', 'syn_unknown_share', 'syn_jaccard'])\n",
    "\n",
    "with open('data/dics/synonyms.pkl', 'rb') as F:\n",
    "    syn_dict = pickle.load(F)\n",
    "\n",
    "syn_features = []\n",
    "p = '_nostops'\n",
    "data = pd.concat((data, \n",
    "                  data.apply(lambda row: syn(row['q1'+p], row['q2'+p]), axis=1)),\n",
    "                  axis=1, copy=False)\n",
    "\n",
    "kagg = pd.concat((kagg, \n",
    "                  kagg.apply(lambda row: syn(row['q1'+p], row['q2'+p]), axis=1)),\n",
    "                  axis=1, copy=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dense features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[features].to_csv(os.path.join(DATA_PATH, 'train_NLP_features.csv'), index=False)\n",
    "kagg[features].to_csv(os.path.join(DATA_PATH,  'test_NLP_features.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information about why among all possible sparse representations, this one was chosen, look in additional_1_choosing_sparse_format.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 50s, sys: 25.6 s, total: 4min 16s\n",
      "Wall time: 5min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p_text = '_stem'\n",
    "corpus = data['q1'+p_text].tolist() + data['q2'+p_text].tolist()\n",
    "tfidf = TfidfVectorizer(max_df=0.8, min_df=3).fit(corpus) \n",
    "\n",
    "data_sparse = sparse.hstack([tfidf.transform(data['inter'+p_text])\n",
    "                            ,tfidf.transform(data['extra'+p_text])\n",
    "                            ], format='csr')\n",
    "kagg_sparse = sparse.hstack([tfidf.transform(kagg['inter'+p_text])\n",
    "                            ,tfidf.transform(kagg['extra'+p_text])\n",
    "                            ], format='csr')                                  \n",
    "\n",
    "p_text = '_tags'\n",
    "corpus = data['q1'+p_text].tolist() + data['q2'+p_text].tolist()\n",
    "tfidf = TfidfVectorizer().fit(corpus) \n",
    "data_sparse = sparse.hstack([data_sparse\n",
    "                            ,tfidf.transform(data['inter'+p_text])\n",
    "                            ,tfidf.transform(data['extra'+p_text])\n",
    "                            ], format='csr')                                  \n",
    "\n",
    "kagg_sparse = sparse.hstack([kagg_sparse\n",
    "                            ,tfidf.transform(kagg['inter'+p_text])\n",
    "                            ,tfidf.transform(kagg['extra'+p_text])\n",
    "                            ], format='csr') \n",
    "del corpus, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename, sparce_matrix):\n",
    "    \"Saves scipy sparse matrix under the given name\"\n",
    "    np.savez(filename, data=sparce_matrix.data, indices=sparce_matrix.indices,\n",
    "             indptr=sparce_matrix.indptr, shape=sparce_matrix.shape)\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    \"Loads scipy sparse matrix with csr format\"\n",
    "    loader = np.load(filename)\n",
    "    return csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])\n",
    "\n",
    "save_sparse_csr('data/data_tfidf_stem_tags.npz', data_sparse)\n",
    "save_sparse_csr('data/kagg_tfidf_stem_tags.npz', kagg_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----- END ------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wordnet similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "data = pd.read_csv(os.path.join(DATA_PATH, 'data_feat1.csv'), sep=';', \n",
    "                   usecols =['target', 'q1_nostops', 'q2_nostops'])\n",
    "data.fillna('xxx', inplace=True)\n",
    "# kagg = pd.read_csv(os.path.join(DATA_PATH, 'kagg_feat_nostops.csv'), sep=';', \n",
    "#                    usecols =['q1_nostops', 'q2_nostops'])\n",
    "# kagg.fillna('xxx', inplace=True)\n",
    "\n",
    "NUM_CORES = 6\n",
    "def apply_parallel(df, my_func):\n",
    "    df_splitted = np.array_split(df, NUM_CORES)\n",
    "    pool = mp.Pool(NUM_CORES)\n",
    "    result = pd.concat(pool.map(my_func, df_splitted))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "def find_similarity(w1,w2, nouns=False, CUT_VALUE=14.50):\n",
    "    if nouns==True:\n",
    "        lst1=wn.synsets(w1,pos=wn.NOUN)\n",
    "        lst2=wn.synsets(w2,pos=wn.NOUN)                          \n",
    "    else:\n",
    "        lst1=wn.synsets(w1)\n",
    "        lst2=wn.synsets(w2)       \n",
    "    \n",
    "    similarities_list=[item1.path_similarity(item2)  for item1 in lst1 \\\n",
    "                                for  item2 in lst2 \\\n",
    "                                if item1.path_similarity(item2)!=None]                     \n",
    "    if len(similarities_list)==0:\n",
    "        max_similarity=0\n",
    "        mean_similarity=0\n",
    "    else:\n",
    "        max_similarity=max(similarities_list)\n",
    "        mean_similarity=np.mean(similarities_list)\n",
    "        \n",
    "    lch_similarities_list=[item1.lch_similarity(item2)  for item1 in lst1 \\\n",
    "                                for  item2 in lst2 \\\n",
    "                                if item1.pos()==item2.pos() and item1.lch_similarity(item2)!=None]                     \n",
    "\n",
    "    if len(lch_similarities_list)==0:\n",
    "        max_lch_similarity=0\n",
    "        mean_lch_similarity=0\n",
    "    else:\n",
    "        max_lch_similarity=max(lch_similarities_list)\n",
    "        mean_lch_similarity=np.mean(lch_similarities_list)\n",
    "        \n",
    "    res_similarities_list=[min(CUT_VALUE,item1.res_similarity(item2,brown_ic))  for item1 in lst1 \\\n",
    "                                for  item2 in lst2 \\\n",
    "                                if item1.pos() not in ['a','s','r'] and item1.pos()==item2.pos() and item1.res_similarity(item2,brown_ic)!=None]  \n",
    "\n",
    "    if len(res_similarities_list)==0:\n",
    "        max_res_similarity=0\n",
    "        mean_res_similarity=0\n",
    "    else:\n",
    "        max_res_similarity=max(res_similarities_list)\n",
    "        mean_res_similarity=np.mean(res_similarities_list)\n",
    "        \n",
    "    return max_similarity, mean_similarity, max_lch_similarity, mean_lch_similarity, max_res_similarity, mean_res_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from time import time\n",
    "\n",
    "def find_similarity(w1,w2, nouns=False, CUT_VALUE=14.50):\n",
    "    if nouns==True:\n",
    "        lst1=wn.synsets(w1,pos=wn.NOUN)\n",
    "        lst2=wn.synsets(w2,pos=wn.NOUN)                          \n",
    "    else:\n",
    "        lst1=wn.synsets(w1)\n",
    "        lst2=wn.synsets(w2)       \n",
    "    \n",
    "    similarities_list=[item1.path_similarity(item2)  for item1 in lst1 \\\n",
    "                                for  item2 in lst2 \\\n",
    "                                if item1.path_similarity(item2)!=None]                     \n",
    "    if len(similarities_list)==0:\n",
    "        max_similarity=0\n",
    "        mean_similarity=0\n",
    "    else:\n",
    "        max_similarity=max(similarities_list)\n",
    "        mean_similarity=np.mean(similarities_list)\n",
    "        \n",
    "    lch_similarities_list=[item1.lch_similarity(item2)  for item1 in lst1 \\\n",
    "                                for  item2 in lst2 \\\n",
    "                                if item1.pos()==item2.pos() and item1.lch_similarity(item2)!=None]                     \n",
    "\n",
    "    if len(lch_similarities_list)==0:\n",
    "        max_lch_similarity=0\n",
    "        mean_lch_similarity=0\n",
    "    else:\n",
    "        max_lch_similarity=max(lch_similarities_list)\n",
    "        mean_lch_similarity=np.mean(lch_similarities_list)\n",
    "        \n",
    "    res_similarities_list=[min(CUT_VALUE,item1.res_similarity(item2,brown_ic))  for item1 in lst1 \\\n",
    "                                for  item2 in lst2 \\\n",
    "                                if item1.pos() not in ['a','s','r'] and item1.pos()==item2.pos() and item1.res_similarity(item2,brown_ic)!=None]  \n",
    "\n",
    "    if len(res_similarities_list)==0:\n",
    "        max_res_similarity=0\n",
    "        mean_res_similarity=0\n",
    "    else:\n",
    "        max_res_similarity=max(res_similarities_list)\n",
    "        mean_res_similarity=np.mean(res_similarities_list)\n",
    "        \n",
    "    return max_similarity, mean_similarity, max_lch_similarity, mean_lch_similarity, max_res_similarity, mean_res_similarity\n",
    "\n",
    "\n",
    "test = data.loc[:100]\n",
    "target = test.target\n",
    "wordnet_sim_dict = dict()\n",
    "t_start = time()\n",
    "counter = 0\n",
    "\n",
    "def wordnet_sim(q1,q2):\n",
    "    global wordnet_sim_dict, counter\n",
    "    counter+=1\n",
    "    results = []\n",
    "    for w1 in set(q1.split(' ')):\n",
    "        for w2 in set(q2.split(' ')):\n",
    "    # for w1 in set(q1.split(' ')) - set(q2.split(' ')):\n",
    "    #     for w2 in set(q2.split(' ')):\n",
    "            if (w1, w2) not in wordnet_sim_dict:\n",
    "                wordnet_sim_dict[(w1,w2)] = find_similarity(w1, w2)\n",
    "                wordnet_sim_dict[(w2,w1)] = wordnet_sim_dict[(w1,w2)]\n",
    "            results.append(wordnet_sim_dict[(w1,w2)])\n",
    "            \n",
    "#     for w1 in set(q2.split(' ')) - set(q1.split(' ')):\n",
    "#         for w2 in set(q1.split(' ')):\n",
    "#             if (w1, w2) not in wordnet_sim_dict:\n",
    "#                 wordnet_sim_dict[(w1,w2)] = find_similarity(w1, w2)\n",
    "#                 wordnet_sim_dict[(w2,w1)] = wordnet_sim_dict[(w1,w2)]\n",
    "#             results.append(wordnet_sim_dict[(w1,w2)])\n",
    "    #print (results)\n",
    "    if len(results)==0:\n",
    "        results = np.array([[1]*6,[1]*6])\n",
    "        #print (results)\n",
    "    else:\n",
    "        results = np.array(results)\n",
    "    if counter % 1000 == 0:\n",
    "        print (counter, len(wordnet_sim_dict), round((time()-t_start)/60,1) ,'minutes')\n",
    "\n",
    "    return pd.Series([np.mean(results[:,0]), np.mean(results[:,1]), \n",
    "                      np.mean(results[:,2]), np.mean(results[:,3]), \n",
    "                      np.mean(results[:,4]), np.mean(results[:,5])\n",
    "                     ], \n",
    "                     index = ['max_similarity', 'mean_similarity', \n",
    "                              'max_lch_similarity', 'mean_lch_similarity', \n",
    "                              'max_res_similarity', 'mean_res_similarity'\n",
    "                             ])\n",
    "test = test.apply(lambda row: wordnet_sim(row.q1_nostops,row.q2_nostops), axis=1)\n",
    "test['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synsets('on the hill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/wordnet_sim_dict.pkl', 'wb') as F:\n",
    "    pickle.dump(wordnet_sim_dict, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from time import time\n",
    "#test = data.loc[:10]\n",
    "wordnet_sim_dict = dict()\n",
    "t_start = time()\n",
    "counter = 0\n",
    "\n",
    "def wordnet_sim(q1,q2):\n",
    "    global wordnet_sim_dict, counter\n",
    "    counter+=1\n",
    "    results = []\n",
    "    for w1 in set(q1.split(' ')):\n",
    "        for w2 in set(q2.split(' ')):\n",
    "            if (w1, w2) not in wordnet_sim_dict:\n",
    "                wordnet_sim_dict[(w1,w2)] = find_similarity(w1, w2)\n",
    "                wordnet_sim_dict[(w2,w1)] = wordnet_sim_dict[(w1,w2)]\n",
    "            results.append(wordnet_sim_dict[(w1,w2)])\n",
    "    results = np.array(results)\n",
    "    if counter % 1000 == 0:\n",
    "        print (counter, len(wordnet_sim_dict), round((time()-t_start)/60,1) ,'minutes')\n",
    "\n",
    "    return pd.Series([np.mean(results[:,0]), np.mean(results[:,1]), np.mean(results[:,2]), \n",
    "                      np.mean(results[:,3]), np.mean(results[:,4]), np.mean(results[:,5])], \n",
    "                     index = ['max_similarity', 'mean_similarity', \n",
    "                              'max_lch_similarity', 'mean_lch_similarity', \n",
    "                              'max_res_similarity', 'mean_res_similarity'])\n",
    "data = data.apply(lambda row: wordnet_sim(row.q1_nostops,row.q2_nostops), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
