{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
    "\n",
    "DATA_PATH = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 46.7 s, total: 3min 1s\n",
      "Wall time: 3min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.concat([pd.read_csv(os.path.join(DATA_PATH, 'train_nonNLP_features.csv')),\n",
    "                   pd.read_csv(os.path.join(DATA_PATH, 'train_NLP_features.csv')),\n",
    "                   pd.read_csv(os.path.join(DATA_PATH, 'train_pos_diff_matrix.csv')),\n",
    "                   pd.read_csv(os.path.join(DATA_PATH, 'train_PN.csv')),\n",
    "                   pd.read_csv(os.path.join(DATA_PATH, 'train_lgb_tfidf_oof.csv')),\n",
    "                   pd.read_csv(os.path.join(DATA_PATH, 'train_nnet_tfidf_oof.csv')),\n",
    "                   pd.read_csv(os.path.join(DATA_PATH, 'train_fm_oof.csv'))\n",
    "                  ], axis=1)\n",
    "\n",
    "test  = pd.concat([pd.read_csv(os.path.join(DATA_PATH, 'test_nonNLP_features.csv')),\n",
    "                   pd.read_csv(os.path.join(DATA_PATH, 'test_NLP_features.csv')),\n",
    "                   pd.read_csv(os.path.join(DATA_PATH, 'test_pos_diff_matrix.csv')),\n",
    "                   pd.read_csv(os.path.join(DATA_PATH, 'test_PN.csv')),\n",
    "                   pd.read_csv(os.path.join(DATA_PATH, 'test_lgb_tfidf_oof.csv')),\n",
    "                   pd.read_csv(os.path.join(DATA_PATH, 'test_nnet_tfidf_oof.csv')),\n",
    "                   pd.read_csv(os.path.join(DATA_PATH, 'test_fm_oof.csv'))\n",
    "                  ], axis=1)\n",
    "\n",
    "ytrain = pd.read_csv(os.path.join(DATA_PATH, 'target.csv')).target\n",
    "ids = pd.read_csv(os.path.join(DATA_PATH, 'train_ids.csv'), usecols=['graph_id'])\n",
    "\n",
    "train.fillna(-1, inplace=True)\n",
    "test .fillna(-1, inplace=True)\n",
    "train.replace([-np.inf, np.inf], -1, inplace=True)\n",
    "test .replace([-np.inf, np.inf], -1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(os.path.join('data', 'bad_features.pkl'), 'rb') as F:\n",
    "        bad_features = pickle.load(F)\n",
    "except:\n",
    "    bad_features = ['q2_char_len_src', 'bigram_all_jaccard_max_src', 'f_Qratio_src', 'pos_diff_end_src', \n",
    "                    'unigram_all_jaccard_max_src', 'unigram_jaccard_stem', 'jac_nostops', 'diffl_stem', \n",
    "                    'jac_stem', 'trigram_all_jaccard_stem', 'trigram_jaccard_src', 'trigram_jaccard_stem', \n",
    "                    'q1_char_len_stem', 'trigram_jaccard_nostops', 'bigram_all_jaccard_max_nostops', \n",
    "                    'unigram_all_jaccard_max_stem', 'unigram_jaccard_nostops', 'loc_q1_country_num', \n",
    "                    'bigram_all_jaccard_max_stem', 'bigram_all_jaccard_stem', 'unigram_all_jaccard_max_nostops', \n",
    "                    'q1_word_len_stem', 'loc_country_match_relative', 'f_Qratio_nostops', 'f_Qratio_stem', \n",
    "                    'unigram_jaccard_src', 'trigram_all_jaccard_max_nostops', 'trigram_all_jaccard_max_stem', \n",
    "                    'jac_src']\n",
    "\n",
    "features = list(set(train.columns) - set(bad_features))\n",
    "print (len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in test.columns:\n",
    "    if test[col].dtypes == 'O':\n",
    "        test[col].replace('xxx', 0., inplace=True)\n",
    "        test[col] = test[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dense features: 185\n",
      "Total number of features: 59811\n",
      "CPU times: user 1min 52s, sys: 2min 31s, total: 4min 24s\n",
      "Wall time: 7min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return sparse.csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                             shape = loader['shape'])\n",
    "\n",
    "def batched_hstack_csr(matrices, batch_size=-1):\n",
    "    \"\"\"\n",
    "    Scipy sparse hstack operation works in linear time only on csc matrices or small csr matrices. \n",
    "    So in this function matrices hstacked by small batches, and then the batches are stacked vertically. Vertical\n",
    "    stacking for csr matrices is very cheap operation.\n",
    "    \n",
    "    Input: \n",
    "        matrices: list of matrices to be hstacked. All must have the same number of rows. \n",
    "            Acceptable formats: csr matrix, pandas DataFrame or numpy array.\n",
    "        batch_size: int, number of rows to hstack per batch. If not defined then batch_size is set equal to \n",
    "            the number of batches, i.e. square root of number of rows in matricies. Recommended batch size 1K - 10K.\n",
    "    Output: scipy csr matrix    \n",
    "    \"\"\"\n",
    "    \n",
    "    if batch_size == -1 or batch_size == 'dynamic':\n",
    "        batch_size = np.ceil(np.sqrt(matrices[0].shape[0]))\n",
    "     \n",
    "    if batch_size == 0:\n",
    "        return sparse.hstack(matrices, format='csr')\n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "    batches = []\n",
    "    for i in range(0, matrices[0].shape[0], batch_size):\n",
    "        lower_bound = i\n",
    "        upper_bound = min(i+batch_size, matrices[0].shape[0])\n",
    "        batches.append(sparse.hstack([matrix[lower_bound:upper_bound] for matrix in matrices]\n",
    "                                     , format='csr'))\n",
    "    \n",
    "    return sparse.vstack(batches, format='csr')\n",
    "\n",
    "\n",
    "train = sparse.csr_matrix(train[features])\n",
    "test  = sparse.csr_matrix(test [features])\n",
    "\n",
    "train_tfidf = load_sparse_csr(os.path.join(DATA_PATH, 'data_tfidf_stem_tags.npz'))\n",
    "test_tfidf  = load_sparse_csr(os.path.join(DATA_PATH, 'kagg_tfidf_stem_tags.npz'))\n",
    "\n",
    "train = batched_hstack_csr([train, train_tfidf])\n",
    "test  = batched_hstack_csr([test,  test_tfidf ])\n",
    "\n",
    "print ('Number of dense features:', len(features))\n",
    "print ('Total number of features:', train.shape[1])\n",
    "\n",
    "del train_tfidf, test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOF blending pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural net definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Convolution1D, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam, SGD, Adadelta\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "def batch_generator(X, y, BATCH_SIZE, EPOCH_PARTION):\n",
    "    \"\"\"\n",
    "    Batch generator for nnet training\n",
    "    input:\n",
    "        X - train dataset, numpy array or csr matrix\n",
    "        y - target, numpy array\n",
    "        BATCH_SIZE - int, number of objects in batch. If X is csr matrix, it will be transformed \n",
    "        to dense array so batch size must be small enough for this array to fit in memory.\n",
    "        EPOCH_PARTION - float. If in interval (0, 1) - share of objects that will be used for training in epoch.\n",
    "            Objects are chosen randomly. If equals to 1 - nnet will be trained on all samples without randomization.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_number = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    batches_per_epoch = np.ceil(X.shape[0]/BATCH_SIZE*EPOCH_PARTION)\n",
    "    \n",
    "    while True:\n",
    "        if EPOCH_PARTION==1:\n",
    "            batch_indexes = sample_index[BATCH_SIZE*batch_number : BATCH_SIZE*(batch_number+1)]    \n",
    "        else:\n",
    "            batch_indexes = np.random.choice(X.shape[0], BATCH_SIZE)\n",
    "        \n",
    "        if type(X).__name__ == 'csr_matrix':\n",
    "            X_batch = X[batch_indexes].toarray()\n",
    "        else:\n",
    "            X_batch = X[batch_indexes]\n",
    "        y_batch = to_categorical(y, num_classes=2)[batch_indexes]\n",
    "        \n",
    "        batch_number += 1\n",
    "        if batch_number == batches_per_epoch-1:\n",
    "            batch_number = 0\n",
    "        yield X_batch, y_batch\n",
    "            \n",
    "def batch_generator_p(X, BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Batch generator for nnet predicitons\n",
    "    input:\n",
    "        X - train dataset,  numpy array or csr matrix\n",
    "        BATCH_SIZE - number of objects in batch. If X is csr matrix, it will be transformed \n",
    "        to dense array so batch size must be small enough for this array to fit in memory        \n",
    "    \"\"\"\n",
    "    batches_per_epoch = np.ceil(X.shape[0]/BATCH_SIZE)\n",
    "    batch_number = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_indexes = sample_index[BATCH_SIZE*batch_number : BATCH_SIZE*(batch_number+1)]\n",
    "        if type(X).__name__ == 'csr_matrix':\n",
    "            X_batch = X[batch_indexes].toarray()\n",
    "        else:\n",
    "            X_batch = X[batch_indexes]\n",
    "        batch_number += 1\n",
    "        yield (X_batch)\n",
    "        if batch_number == batches_per_epoch:\n",
    "            batch_number = 0\n",
    "            \n",
    "def compile_nnet(data, n1, n2, d1, d2, regul, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to compile simple nnet. Architecture is self-explanatory with code\n",
    "    input:\n",
    "        data - numpy arary or csr matrix for training\n",
    "        n1, n2 - ints, number of neurons in first and second layers\n",
    "        d1, d2 - float, dropouts in first and second layers\n",
    "        regul - float, regularization paramter, the same for both layers\n",
    "        parameters might be passed as a dictionary\n",
    "    output:\n",
    "        nnet model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    if regul>0:\n",
    "        model.add(Dense(n1, input_dim=data.shape[1], \n",
    "                        kernel_regularizer=l2(regul), \n",
    "                        activity_regularizer=l1(regul)))\n",
    "    else:    \n",
    "        model.add(Dense(n1, input_dim=data.shape[1]))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(d1))\n",
    "    \n",
    "    if regul>0:\n",
    "        model.add(Dense(n2, \n",
    "                        kernel_regularizer=l2(regul), \n",
    "                        activity_regularizer=l1(regul)))\n",
    "    else:    \n",
    "        model.add(Dense(n2))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(d2))\n",
    "\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = Adam()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['binary_crossentropy'])\n",
    "    return model\n",
    "\n",
    "def nnet_pred(params, train, ytrain, valid, yvalid, test_fold, kagg):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        params - dictionary of parameters to be passed to function compile_nnet. May also contain size of a \n",
    "            batch and share of objects per epoch\n",
    "        train, valid, test_fold, kagg - numpy arrays or csr matrices. Nnet is trained on train data, best \n",
    "            number of epochs is chosen by binary_crossentropy loss on valid. Best model is saved every epoch \n",
    "            and is loaded if there was no improvement on valid set for 10 epochs in a row. Test_fold and \n",
    "            kagg - matrices, for which predictions are returned.\n",
    "        ytrain, yvalid - 1-dim numpy arrays, labels for train and valid sets.\n",
    "    output: two 1-dim numpy arrays with predicted positive class probability for test_fold and kagg datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(train).__name__ == 'csr_matrix':\n",
    "        scaler = MaxAbsScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "    train = scaler.fit_transform(train)\n",
    "    valid = scaler.transform(valid)\n",
    "    test_fold  = scaler.transform(test_fold)\n",
    "    kagg  = scaler.transform(kagg)\n",
    "    \n",
    "    model = compile_nnet(train, **params)\n",
    "    early_stopper = EarlyStopping(monitor='val_binary_crossentropy', patience=10, verbose=0, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(filepath='nnet_checkpoint.hdf5', \n",
    "                                 monitor='val_binary_crossentropy', \n",
    "                                 save_best_only=True)\n",
    "\n",
    "    BATCH_SIZE = params.get('BATCH_SIZE', 256)\n",
    "    EPOCH_PARTION = params.get('EPOCH_PARTION', 1)\n",
    "        \n",
    "    model.fit_generator(generator=batch_generator(train, ytrain, BATCH_SIZE, EPOCH_PARTION),\n",
    "                        samples_per_epoch=np.ceil(train.shape[0]/BATCH_SIZE*EPOCH_PARTION),\n",
    "                        verbose=0, nb_epoch=1000,\n",
    "\n",
    "                        validation_data=batch_generator(valid, yvalid, BATCH_SIZE, EPOCH_PARTION), \n",
    "                        validation_steps = int(valid.shape[0]/BATCH_SIZE),\n",
    "\n",
    "                        callbacks=[early_stopper, checkpoint])\n",
    "    \n",
    "    model.load_weights('nnet_checkpoint.hdf5') \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['binary_crossentropy'])\n",
    "    \n",
    "    fold_pred = model.predict_generator(generator=batch_generator_p(test_fold, BATCH_SIZE), \n",
    "                                        val_samples=test_fold.shape[0]/BATCH_SIZE)\n",
    "    kagg_pred = model.predict_generator(generator=batch_generator_p(kagg, BATCH_SIZE), \n",
    "                                        val_samples=kagg.shape[0]/BATCH_SIZE)\n",
    "    \n",
    "    return fold_pred[:,1], kagg_pred[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "def xgb_pred(params, train, ytrain, valid, yvalid, test_fold, kagg):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        params - dictionary of parameters to be passed to xgb.train\n",
    "        train, valid, test_fold, kagg - numpy arrays or csr matrices. Model is trained on train data, best \n",
    "            number of epochs is chosen by loss on valid. Test_fold and kagg - matrices, for which predictions \n",
    "            are returned.\n",
    "        ytrain, yvalid - 1-dim numpy arrays, labels for train and valid sets.\n",
    "    output: two 1-dim numpy arrays with predicted positive class probability for test_fold and kagg datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    train  = xgb.DMatrix(train, ytrain)\n",
    "    dvalid = xgb.DMatrix(valid, yvalid)\n",
    "    watchlist = [(train, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "    boost = xgb.train(params, train, \n",
    "                    num_boost_round=10000, \n",
    "                    evals=watchlist,\n",
    "                    verbose_eval=False,\n",
    "                    early_stopping_rounds=20)\n",
    "    \n",
    "    # if we trained a linear model, then it has no ntree_limit parameter\n",
    "    if params['booster'] == 'gbtree':\n",
    "        fold_pred = boost.predict(xgb.DMatrix(test_fold), ntree_limit=boost.best_iteration)\n",
    "        kagg_pred = boost.predict(xgb.DMatrix(kagg),      ntree_limit=boost.best_iteration)\n",
    "    else:\n",
    "        fold_pred = boost.predict(xgb.DMatrix(test_fold))\n",
    "        kagg_pred = boost.predict(xgb.DMatrix(kagg))\n",
    "        \n",
    "    return fold_pred, kagg_pred\n",
    "\n",
    "def lgb_pred(params, train, ytrain, valid, yvalid, test_fold, kagg):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        params - dictionary of parameters to be passed to lgb.train\n",
    "        train, valid, test_fold, kagg - numpy arrays or csr matrices. Model is trained on train data, best \n",
    "            number of epochs is chosen by loss on valid. Test_fold and kagg - matrices, for which predictions \n",
    "            are returned.\n",
    "        ytrain, yvalid - 1-dim numpy arrays, labels for train and valid sets.\n",
    "    output: two 1-dim numpy arrays with predicted positive class probability for test_fold and kagg datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    train  = lgb.Dataset(train, ytrain)\n",
    "    dvalid = lgb.Dataset(valid, yvalid, reference=train)\n",
    "    \n",
    "    gbm = lgb.train(params, train,\n",
    "                    num_boost_round=100000,\n",
    "                    valid_sets=[train, dvalid],\n",
    "                    verbose_eval=False,\n",
    "                    early_stopping_rounds=20)\n",
    "    \n",
    "    fold_pred = gbm.predict(test_fold, num_iteration=gbm.best_iteration)\n",
    "    kagg_pred = gbm.predict(kagg,      num_iteration=gbm.best_iteration) \n",
    "    return fold_pred, kagg_pred\n",
    "\n",
    "def fastfm_pred(params, train, ytrain, test_fold, kagg):\n",
    "    fmc = FMClassification(**params)\n",
    "    fmc.fit(train, ytrain)\n",
    "    fold_pred = fmc.predict_proba(test)\n",
    "    kagg_pred = fmc.predict_proba(kagg)\n",
    "    return fold_pred, kagg_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_oofs(model, data, kagg, y, ids, n_splits=5, iters_total=3, params=None):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        model: string ('lgb'/'fastfm'/'xgb'/'nnet') or any sklearn model\n",
    "        data: train data in format of pandas DataFrame / numpay array / csr sparse matrix. All columns will be \n",
    "            used as features. Must not contain target or ids or non-numeric columns\n",
    "        kagg: test data in format of pandas DataFrame / numpay array / csr sparse matrix. All columns will be \n",
    "            used as features. Must not contain target or ids or non-numeric columns\n",
    "        y: target for train data in a form of pandas DataFrame / pandas Series / numpay array\n",
    "        ids: graph ids in a form of pandas Series. Graphs ids are used to split train data into separate graphs \n",
    "            to prevent overfitting. In other words, quesitons of the same graph will always be in one fold.\n",
    "        n_splits: number of splits for train data. Default value is 5. In order to get one OOF prediciton, \n",
    "            model must be fitted n_splits times\n",
    "        iters_total: number of total iterations. Default value is 3. OOFs then will be blended. \n",
    "            Total number of times a model will be fitted is n_splits*iters_total.\n",
    "        params: model parameters in a form of dictionary.\n",
    "    output: \n",
    "        data_oofs: numpy array of OOF predictions for train data. Result is blended iters_total times\n",
    "        kagg_oofs: numpy array of predictions for train data. Result is blended n_splits*iters_total times\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(data).__name__=='DataFrame':\n",
    "        data = data.values\n",
    "    if type(kagg).__name__=='DataFrame':\n",
    "        kagg = kagg.values\n",
    "    if model == 'fastfm':\n",
    "        y = y.replace(0, -1)\n",
    "        if type(data).__name__ != 'csr_matrix':\n",
    "            data = sparse.csr_matrix(data)\n",
    "            kagg = sparse.csr_matrix(kagg)\n",
    "    if type(y).__name__=='Series' or type(y).__name__=='DataFrame':\n",
    "        y = y.values\n",
    "    \n",
    "    # matrices to store preditions\n",
    "    data_oofs = np.zeros((data.shape[0]))\n",
    "    kagg_oofs = np.zeros((kagg.shape[0]))\n",
    "    \n",
    "    graph_ids_unique = ids.graph_id.unique()\n",
    "    \n",
    "    for iter_num in range(iters_total):\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "        for train_graphs, test_graphs  in kf.split(graph_ids_unique):\n",
    "            train_ind = ids[ids.graph_id.isin(graph_ids_unique[train_graphs])].index.values\n",
    "            test_ind  = ids[ids.graph_id.isin(graph_ids_unique[test_graphs ])].index.values\n",
    "            \n",
    "            # Adding validation sets (from train set) for models, that require it. \n",
    "            # Validation size is 12.5% of train fold.\n",
    "            if model=='xgb' or model=='lgb' or model=='nnet':\n",
    "                kf_valid = KFold(n_splits=8, shuffle=True)\n",
    "                graph_ids_train = graph_ids_unique[train_graphs]\n",
    "                train_graphs, valid_graphs = list(kf_valid.split(graph_ids_train))[0]\n",
    "                train_ind = ids[ids.graph_id.isin(graph_ids_train[train_graphs])].index.values\n",
    "                valid_ind = ids[ids.graph_id.isin(graph_ids_train[valid_graphs])].index.values\n",
    "                \n",
    "            if model=='lgb':\n",
    "                fold_pred, kagg_pred = lgb_pred (params, data[train_ind], y[train_ind], \n",
    "                                                         data[valid_ind], y[valid_ind],\n",
    "                                                         data[test_ind], kagg)\n",
    "            elif model=='xgb':\n",
    "                fold_pred, kagg_pred = xgb_pred (params, data[train_ind], y[train_ind], \n",
    "                                                         data[valid_ind], y[valid_ind],\n",
    "                                                         data[test_ind], kagg)\n",
    "            elif model=='nnet':\n",
    "                fold_pred, kagg_pred = nnet_pred(params, data[train_ind], y[train_ind], \n",
    "                                                         data[valid_ind], y[valid_ind],\n",
    "                                                         data[test_ind], kagg)\n",
    "            elif model=='fastfm':\n",
    "                fold_pred, kagg_pred = fastfm_pred(params, data[train_ind], y[train_ind], data[test_ind], kagg)\n",
    "               \n",
    "            # Block for working with sklearn models\n",
    "            else:\n",
    "                model.fit(data[train_ind], y[train_ind])\n",
    "                try:\n",
    "                    fold_pred = model.predict_proba(data[test_ind])[:,1]\n",
    "                    kagg_pred = model.predict_proba(kagg)[:,1]\n",
    "                except:\n",
    "                    try:\n",
    "                        fold_pred = model.predict_proba(data[test_ind])\n",
    "                        kagg_pred = model.predict_proba(kagg)\n",
    "                    except:\n",
    "                        fold_pred = model.predict(data[test_ind])\n",
    "                        kagg_pred = model.predict(kagg)\n",
    "            \n",
    "            \n",
    "            data_oofs[test_ind] += fold_pred\n",
    "            kagg_oofs += kagg_pred\n",
    "            print ('fold loss: ', log_loss(y[test_ind], fold_pred))            \n",
    "        \n",
    "        print ('iteration OOF score:', log_loss(y, data_oofs/(iter_num+1)))\n",
    "    data_oofs /= iters_total\n",
    "    kagg_oofs /= (iters_total*n_splits)\n",
    "    return data_oofs, kagg_oofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold loss:  0.206652461036\n",
      "fold loss:  0.210304145792\n",
      "fold loss:  0.211509647168\n",
      "fold loss:  0.210966099118\n",
      "fold loss:  0.20803044701\n",
      "iteration OOF score: 0.209479414098\n",
      "fold loss:  0.204708527536\n",
      "fold loss:  0.214105014042\n",
      "fold loss:  0.213041694471\n",
      "fold loss:  0.213451965665\n",
      "fold loss:  0.204162369423\n",
      "iteration OOF score: 0.208514718953\n",
      "fold loss:  0.211135022158\n",
      "fold loss:  0.215385021926\n",
      "fold loss:  0.2054668681\n",
      "fold loss:  0.204049653532\n",
      "fold loss:  0.211985623477\n",
      "iteration OOF score: 0.208117206419\n",
      "0.208117206419\n",
      "CPU times: user 4h 54min 52s, sys: 16min 38s, total: 5h 11min 30s\n",
      "Wall time: 1h 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_preds = pd.DataFrame()\n",
    "test_preds  = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    with open(os.path.join('data', 'lgb_best_params.pkl'), 'rb') as F:\n",
    "        params = pickle.load(F)\n",
    "except:\n",
    "    pass\n",
    "    params = {'bagging_fraction': 0.9,\n",
    "             'bagging_freq': 1,\n",
    "             'boosting_type': 'gbdt',\n",
    "             'feature_fraction': 0.85,\n",
    "             'lambda_l2': 1,\n",
    "             'learning_rate': 0.05,\n",
    "             'max_bin': 500,\n",
    "             'metric': 'binary_logloss',\n",
    "             'min_data_in_leaf': 71,\n",
    "             'min_gain_to_split': 0,\n",
    "             'num_leaves': 32,\n",
    "             'objective': 'binary',\n",
    "             'task': 'train'}\n",
    "\n",
    "train_preds['lgb_oof'], test_preds['lgb_oof'] = get_oofs('lgb'\n",
    "                                                        ,train\n",
    "                                                        ,test\n",
    "                                                        ,ytrain\n",
    "                                                        ,ids \n",
    "                                                        ,params=params)\n",
    "print (log_loss(ytrain, train_preds.lgb_oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_preds.to_csv(os.path.join(DATA_PATH, 'train_preds.csv'), index=False)\n",
    "test_preds .to_csv(os.path.join(DATA_PATH, 'test_preds.csv'),  index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
